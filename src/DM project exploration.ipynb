{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import ceil\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from datetime import datetime\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from itertools import product\n",
    "from pandas_profiling import ProfileReport\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataset into a variable called donors\n",
    "donors = pd.read_csv('donors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#getting some information about the dataset\n",
    "donors.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a copy of the dataset\n",
    "data = donors.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the correlation between two columns that seem to be showing the same data \n",
    "#as the values are high, we can drop one set of data points as the data is redundant \n",
    "print(spearmanr(data['MALEMILI'], data['AFC2']))\n",
    "print(spearmanr(data['MALEVET'], data['AFC5']))\n",
    "print(spearmanr(data['VIETVETS'], data['VC1']))\n",
    "print(spearmanr(data['WWIIVETS'], data['VC3']))\n",
    "print(spearmanr(data['LOCALGOV'], data['OEDC1']))\n",
    "print(spearmanr(data['STATEGOV'], data['OEDC2']))\n",
    "print(spearmanr(data['FEDGOV'], data['OEDC3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spearmanr(data['POP901'], data['POP902']))\n",
    "print(spearmanr(data['POP901'], data['POP903']))\n",
    "print(spearmanr(data['POP902'], data['POP903']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting only columns with metric data\n",
    "metric_data = data.select_dtypes(include=np.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting the metric features and non metric features into lists\n",
    "metric_features = data.select_dtypes(include=np.number).columns.tolist()\n",
    "non_metric_features = data.columns.drop(metric_features).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(metric_features))\n",
    "print(len(non_metric_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neighbourhood data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# give column name \n",
    "col_name_1 = \"POP901\"\n",
    "col_name_2 = \"ADATE_2\"\n",
    "col_name_3 = \"MALEMILI\"\n",
    "col_name_4 = \"SOLP3\"\n",
    "col_name_5 = \"CONTROLN\"\n",
    "  \n",
    "# find the index no \n",
    "index_no_1 = data.columns.get_loc(col_name_1)\n",
    "index_no_2 = data.columns.get_loc(col_name_2)\n",
    "index_no_3 = data.columns.get_loc(col_name_3)\n",
    "index_no_4 = data.columns.get_loc(col_name_4)\n",
    "index_no_5 = data.columns.get_loc(col_name_5)\n",
    "\n",
    "#creating data frame for only the neighbourhood data \n",
    "nbh_data = data.iloc[:, index_no_1:index_no_2]\n",
    "nbh_data_2 = data.iloc[:,index_no_3:index_no_4]\n",
    "df_control = data.iloc[:,index_no_5]\n",
    "\n",
    "df_neighbours = nbh_data.join(nbh_data_2)\n",
    "ndf = df_neighbours.join(df_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.drop(columns=['MSA','ADI','DMA'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nbh_corrMatrix = ndf.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of metric features \n",
    "nbh_metric_features = ndf.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ndf.shape[1]/len(metric_features)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sectioning neighbourhood data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Veterans1 = ndf.iloc[:, ndf.columns.get_loc('MALEMILI'):ndf.columns.get_loc('LOCALGOV')]\n",
    "Veterans2 = ndf.iloc[:, ndf.columns.get_loc('AFC1'):ndf.columns.get_loc('ANC1')]\n",
    "Population = ndf.iloc[:, ndf.columns.get_loc('POP901'):ndf.columns.get_loc('ETH1')]\n",
    "Age1 = ndf.iloc[:, ndf.columns.get_loc('AGE901'):ndf.columns.get_loc('HHAGE1')]\n",
    "Age2 = ndf.iloc[:, ndf.columns.get_loc('AC1'):ndf.columns.get_loc('MALEMILI')]\n",
    "Movement = ndf.iloc[:, ndf.columns.get_loc('MC1'):ndf.columns.get_loc('TPE1')]\n",
    "Ethnicity1 = ndf.iloc[:, ndf.columns.get_loc('ETH1'):ndf.columns.get_loc('AGE901')]\n",
    "Ethnicity2 = ndf.iloc[:, ndf.columns.get_loc('ETHC1'):ndf.columns.get_loc('HVP1')]\n",
    "Ancestry = ndf.iloc[:, ndf.columns.get_loc('ANC1'):ndf.columns.get_loc('VOC1')]\n",
    "Household1 = ndf.iloc[:, ndf.columns.get_loc('HHAGE1'):ndf.columns.get_loc('ETHC1')]\n",
    "Household2 = ndf.iloc[:, ndf.columns.get_loc('HVP1'):ndf.columns.get_loc('IC1')]\n",
    "Household3 = ndf.iloc[:, ndf.columns.get_loc('VOC1'):ndf.columns.get_loc('AC1')]\n",
    "Income = ndf.iloc[:, ndf.columns.get_loc('IC1'):ndf.columns.get_loc('MC1')]\n",
    "Transportation = ndf.iloc[:, ndf.columns.get_loc('TPE1'):ndf.columns.get_loc('LFC1')]\n",
    "Employment1 = ndf.iloc[:, ndf.columns.get_loc('OCC1'):ndf.columns.get_loc('EC1')]\n",
    "Employment2 = ndf.iloc[:, ndf.columns.get_loc('LOCALGOV'):ndf.columns.get_loc('CONTROLN')]\n",
    "LabourForce = ndf.iloc[:, ndf.columns.get_loc('LFC1'):ndf.columns.get_loc('OCC1')]\n",
    "Education = ndf.iloc[:, ndf.columns.get_loc('EC1'):ndf.columns.get_loc('AFC1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Veterans = Veterans1.join(Veterans2).join(df_control)\n",
    "Population = Population.join(Movement).join(df_control)\n",
    "Age = Age1.join(Age2).join(df_control)\n",
    "Ethnicity = Ethnicity1.join(Ethnicity2).join(df_control)\n",
    "Ancestry = Ancestry.join(df_control)\n",
    "Household = Household1.join(Household2).join(Household3).join(df_control)\n",
    "Income = Income.join(df_control)\n",
    "Transportation = Transportation.join(df_control)\n",
    "Employment = Employment1.join(Employment2).join(df_control)\n",
    "LabourForce = LabourForce.join(df_control)\n",
    "Education = Education.join(df_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_impute = ['POP90C4','POP90C5','AGE901','AGE902','AGE903', 'AGE904','AGE905','AGE906','AGE907','CHIL1','CHIL2','CHIL3','AGEC1','AGEC2','AGEC3','AGEC4','AGEC5','CHILC1','CHILC2','CHILC3','CHILC4','CHILC5','HHN1','HHN2','HHN3','HHN4','MARR1','HHP1','HHP2','HU1','HU3','HHD1','HHD2','HHD3','HHD4','HHD5','ETHC1','ETHC2','RHP1','RHP2','RP4','IC1','IC2','IC3','IC4','HHAS3','MC1','MC2','TPE1','TPE13','LFC1','LFC2','LFC3','LFC4','LFC5','OEDC5','EC4','AFC5','VC1','VC2','VC3','VC4','POBC2','VOC2','MHUC1','MHUC2','MALEVET','VIETVETS','WWIIVETS']\n",
    "dataframes = [Veterans,Population,Age,Ethnicity,Ancestry,Household,Income,Transportation,Employment,LabourForce,Education]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def impute_values(df):\n",
    "    for column in df.columns.to_list():\n",
    "        if column in columns_impute:\n",
    "            df[column].replace(0, np.nan, inplace=True)\n",
    "            # KNNImputer or SimpleImputer\n",
    "            imputer = KNNImputer()\n",
    "            df[df.columns.to_list()] = imputer.fit_transform(df[df.columns.to_list()])\n",
    "            \n",
    "def treat_outliers(df, m):\n",
    "    q25 = df.quantile(.25)\n",
    "    q75 = df.quantile(.75)\n",
    "    iqr = (q75 - q25)\n",
    "\n",
    "    upper_lim = q75 + m * iqr\n",
    "    lower_lim = q25 - m * iqr\n",
    "\n",
    "    filters = []\n",
    "    for metric in df.columns.to_list():\n",
    "        llim = lower_lim[metric]\n",
    "        ulim = upper_lim[metric]\n",
    "        filters.append(df[metric].between(llim, ulim, inclusive=True))\n",
    "\n",
    "    df_2 = df[np.all(filters, 0)]\n",
    "    print('Percentage of data kept after removing outliers:', np.round(df_2.shape[0] / df.shape[0], 4))\n",
    "    \n",
    "def plot_dist(df):\n",
    "    # All Numeric Variables' Histograms in one figure\n",
    "    sns.set()\n",
    "\n",
    "    # Prepare figure. Create individual axes where each histogram will be placed\n",
    "    fig, axes = plt.subplots(2, ceil(len(df.columns.to_list()) / 2), figsize=(20, 11))\n",
    "\n",
    "    # Plot data\n",
    "    # Iterate across axes objects and associate each histogram (hint: use the ax.hist() instead of plt.hist()):\n",
    "    for ax, feat in zip(axes.flatten(), df.columns.to_list()): # Notice the zip() function and flatten() method\n",
    "        ax.hist(df[feat])\n",
    "        ax.set_title(feat, y=-0.13)\n",
    "    \n",
    "    # Layout\n",
    "    # Add a centered title to the figure:\n",
    "    title = \"Numeric Variables' Histograms\"\n",
    "\n",
    "    plt.suptitle(title)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_cluster = [Population, Age, Income, Transportation, Education, Ethnicity, Veterans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dataframes:\n",
    "    df.set_index('CONTROLN', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for df in dataframes_cluster:\n",
    "    impute_values(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Household.set_index('CONTROLN', inplace = True)\n",
    "Employment.set_index('CONTROLN', inplace = True)\n",
    "LabourForce.set_index('CONTROLN', inplace = True)\n",
    "Ancestry.set_index('CONTROLN', inplace = True)\n",
    "Population.set_index('CONTROLN', inplace = True)\n",
    "Age.set_index('CONTROLN', inplace = True)\n",
    "Education.set_index('CONTROLN', inplace = True)\n",
    "Income.set_index('CONTROLN', inplace = True)\n",
    "Transportation.set_index('CONTROLN', inplace = True)\n",
    "Ethnicity.set_index('CONTROLN', inplace = True)\n",
    "Veterans.set_index('CONTROLN', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed = Population.join(Age).join(Income).join(Transportation).join(Education).join(Ethnicity).join(Veterans)                                                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list of columns that are not percentages\n",
    "non_percentage = ['POP901','POP902','POP903','AGE901','AGE902','AGE903','AGE904','AGE905','AGE906','HHP1','HHP2','HV1','HV2','HV3','HV4','RHP1','RHP2','RHP3','RHP4','MHUC1','MHUC2','IC1','IC2','IC3','IC4','IC5','TPE10','TPE11','EC1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(df):\n",
    "    metric_feat = df.columns.to_list()\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    to_scale = []\n",
    "    for feature in metric_feat:\n",
    "        if feature not in non_percentage:\n",
    "            df[feature] = df[feature]/100\n",
    "        elif feature in non_percentage:\n",
    "            to_scale.append(feature)\n",
    "\n",
    "    if len(to_scale) > 0:\n",
    "        df[to_scale] = scaler.fit_transform(df[to_scale])\n",
    "\n",
    "def pca_analysis(df):\n",
    "    df_pca = df.copy()\n",
    "    metric_feat = df_pca.columns.to_list()\n",
    "    pca = PCA()\n",
    "    pca_feat = pca.fit_transform(df_pca[metric_feat])\n",
    "    \n",
    "    \n",
    "    #how many principal components to retain\n",
    "    # Output PCA table\n",
    "    pca_result = pd.DataFrame(\n",
    "        {\"Eigenvalue\": pca.explained_variance_,\n",
    "         \"Difference\": np.insert(np.diff(pca.explained_variance_), 0, 0),\n",
    "         \"Proportion\": pca.explained_variance_ratio_,\n",
    "         \"Cumulative\": np.cumsum(pca.explained_variance_ratio_)},\n",
    "        index=range(1, pca.n_components_ + 1)\n",
    "        )\n",
    "    print(pca_result.head(25))\n",
    "    \n",
    "    # figure and axes\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # draw plots\n",
    "    ax1.plot(pca.explained_variance_, marker=\".\", markersize=12)\n",
    "    ax2.plot(pca.explained_variance_ratio_, marker=\".\", markersize=12, label=\"Proportion\")\n",
    "    ax2.plot(np.cumsum(pca.explained_variance_ratio_), marker=\".\", markersize=12, linestyle=\"--\", label=\"Cumulative\")\n",
    "\n",
    "    # customizations\n",
    "    ax2.legend()\n",
    "    ax1.set_title(\"Scree Plot\", fontsize=14)\n",
    "    ax2.set_title(\"Variance Explained\", fontsize=14)\n",
    "    ax1.set_ylabel(\"Eigenvalue\")\n",
    "    ax2.set_ylabel(\"Proportion\")\n",
    "    ax1.set_xlabel(\"Components\")\n",
    "    ax2.set_xlabel(\"Components\")\n",
    "    ax1.set_xticks(range(0, pca.n_components_, 2))\n",
    "    ax1.set_xticklabels(range(1, pca.n_components_ + 1, 2))\n",
    "    ax2.set_xticks(range(0, pca.n_components_, 2))\n",
    "    ax2.set_xticklabels(range(1, pca.n_components_ + 1, 2))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "#interpreting each principal component\n",
    "def _color_red_or_green(val):\n",
    "    if val < -0.45:\n",
    "        color = 'background-color: red'\n",
    "    elif val > 0.45:\n",
    "        color = 'background-color: green'\n",
    "    else:\n",
    "        color = ''\n",
    "    return color\n",
    "\n",
    "# Perform PCA again with the number of principal components you want to retain\n",
    "def pca_describe(df,n_components):\n",
    "    pca = PCA(n_components)\n",
    "    df_pca = df.copy()\n",
    "    metric_feat = df_pca.columns.to_list()\n",
    "    pca_feat = pca.fit_transform(df_pca[metric_feat])\n",
    "    pca_feat_names = [f\"PC{i}\" for i in range(pca.n_components_)]\n",
    "    pca_df = pd.DataFrame(pca_feat, index=df_pca.index, columns=pca_feat_names)  # remember index=df_pca.index\n",
    "    # Reassigning df to contain pca variables\n",
    "    df_pca = pd.concat([df_pca, pca_df], axis=1)\n",
    "\n",
    "    # Interpreting each Principal Component\n",
    "    loadings = df_pca[metric_feat + pca_feat_names].corr().loc[metric_feat, pca_feat_names]\n",
    "    return loadings.style.applymap(_color_red_or_green)\n",
    "\n",
    "def pca_apply(df,n_components):\n",
    "    pca = PCA(n_components)\n",
    "    df_pca = df.copy()\n",
    "    metric_feat = df_pca.columns.to_list()\n",
    "    pca_feat = pca.fit_transform(df_pca[metric_feat])\n",
    "    pca_feat_names = [f\"PC{i}\" for i in range(pca.n_components_)]\n",
    "    pca_df = pd.DataFrame(pca_feat, index=df_pca.index, columns=pca_feat_names)  # remember index=df_pca.index\n",
    "    return pca_df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [Veterans,Population,Age,Ethnicity,Ancestry,Household,Income,Transportation,Employment,LabourForce,Education]\n",
    "dataframes = [prep_data(df) for df in dataframes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [Veterans,Population,Age,Ethnicity,Ancestry,Household,Income,Transportation,Employment,LabourForce,Education]\n",
    "dataframes_names = ['Veterans','Population','Age','Ethnicity','Ancestry','Household','Income','Transportation','Employment','LabourForce','Education']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for df, name in zip(dataframes, dataframes_names):\n",
    "    df.name = name \n",
    "    print(df.name)\n",
    "    pca_analysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [Veterans,Population,Age,Ethnicity,Ancestry,Household,Income,Transportation,Employment,LabourForce,Education]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pca_describe(Veterans,6) # + correl with WWII vets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_describe(Population,6) # PC0 + correlated with rural area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_describe(Age,6) # positive correlated with higher age of pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_describe(Ethnicity,6) # negatively correlated with white population and positively correlated with black pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_describe(Ancestry,6) # difficult to understand the PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_describe(Household,6) # difficult to understand the PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_describe(Income,6) # + correlated with higher income "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_describe(Transportation,6) # positively correlated with time to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_describe(Employment,6) # difficult to understand the PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_describe(LabourForce,6) # difficult to understand the PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_describe(Education,6) # positively correlated with education level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Pop_PC = pca_apply(Population,1)\n",
    "Pop_PC.rename(columns={\"PC0\": \"Population\"},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Age_PC = pca_apply(Age,1) #check the correlation with age \n",
    "Age_PC.rename(columns={\"PC0\": \"Age\"},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Education_PC = pca_apply(Education,1) # Median year of school completed, % 25+ with bachelors/graduate degree (- correlated with higher education)\n",
    "Education_PC.rename(columns={\"PC0\": \"Education\"},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Income_PC = pca_apply(Income,1) # household income (PC0 pn[ 0+ correlated with higher income)\n",
    "Income_PC.rename(columns={\"PC0\": \"Income\"},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transportation_PC = pca_apply(Transportation,1) #time to work\n",
    "Transportation_PC.rename(columns={\"PC0\": \"Transport\"},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ethnicity_PC = pca_apply(Ethnicity,1) #time to work\n",
    "Ethnicity_PC.rename(columns={\"PC0\": \"Ethnicity\"},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Veterans_PC = pca_apply(Veterans,1)\n",
    "Veterans_PC.rename(columns={\"PC0\": \"Veterans\"},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cluster = Pop_PC.join(Age_PC).join(Education_PC).join(Income_PC).join(Transportation_PC).join(Ethnicity_PC).join(Veterans_PC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_feat_cluster = Population.join(Age).join(Education).join(Income).join(Transportation).join(Veterans).join(Ethnicity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cluster.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treat_outliers(data_cluster,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cluster_2.shape[0]/data_cluster.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = data_cluster[~data_cluster.isin(data_cluster_2)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other = Household.join(Employment).join(LabourForce).join(Ancestry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_analysis(other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_apply(other,6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(data_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.predict(data_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_clusters = range(1, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "for n_clus in range_clusters:  # iterate over desired ncluster range\n",
    "    kmclust = KMeans(n_clusters=n_clus, init='k-means++', n_init=15, random_state=1)\n",
    "    kmclust.fit(data_cluster)\n",
    "    inertia.append(kmclust.inertia_)  # save the inertia of the given cluster solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inertia plot\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(inertia)\n",
    "plt.ylabel(\"Inertia: SSw\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.title(\"Inertia plot over clusters\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py\n",
    "\n",
    "# Storing average silhouette metric\n",
    "avg_silhouette = []\n",
    "for nclus in range_clusters:\n",
    "    # Skip nclus == 1\n",
    "    if nclus == 1:\n",
    "        continue\n",
    "    \n",
    "    # Create a figure\n",
    "    fig = plt.figure(figsize=(13, 7))\n",
    "\n",
    "    # Initialize the KMeans object with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    kmclust = KMeans(n_clusters=nclus, init='k-means++', n_init=15, random_state=1)\n",
    "    cluster_labels = kmclust.fit_predict(data_cluster)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed clusters\n",
    "    silhouette_avg = silhouette_score(data_cluster, cluster_labels)\n",
    "    avg_silhouette.append(silhouette_avg)\n",
    "    print(f\"For n_clusters = {nclus}, the average silhouette_score is : {silhouette_avg}\")\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(data_cluster, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(nclus):\n",
    "        # Aggregate the silhouette scores for samples belonging to cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        \n",
    "        # Get y_upper to demarcate silhouette y range size\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        \n",
    "        # Filling the silhouette\n",
    "        color = cm.nipy_spectral(float(i) / nclus)\n",
    "        plt.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    plt.title(\"The silhouette plot for the various clusters.\")\n",
    "    plt.xlabel(\"The silhouette coefficient values\")\n",
    "    plt.ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    plt.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    \n",
    "    # The silhouette coefficient can range from -1, 1\n",
    "    xmin, xmax = np.round(sample_silhouette_values.min() -0.1, 2), np.round(sample_silhouette_values.max() + 0.1, 2)\n",
    "    plt.xlim([xmin, xmax])\n",
    "    \n",
    "    # The (nclus+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    plt.ylim([0, len(data_cluster) + (nclus + 1) * 10])\n",
    "\n",
    "    plt.yticks([])  # Clear the yaxis labels / ticks\n",
    "    plt.xticks(np.arange(xmin, xmax, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The average silhouette plot\n",
    "# The inertia plot\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(avg_silhouette)\n",
    "plt.ylabel(\"Average silhouette\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.title(\"Average silhouette plot over clusters\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering_names = ['MiniBatchKMeans', 'AffinityPropagation', 'MeanShift','SpectralClustering', 'Ward', 'AgglomerativeClustering','DBSCAN', 'Birch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on all neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_data(ndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pca=ndf.copy()\n",
    "# Use PCA to reduce dimensionality of data\n",
    "pca = PCA()\n",
    "pca_feat = pca.fit_transform(df_pca[nbh_metric_features])\n",
    "pca_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the projected observations on the principal components axes (linear combinations)\n",
    "pd.DataFrame(df_pca[nbh_metric_features].values @ pca.components_.T, \n",
    "             index=df_pca.index,\n",
    "             columns=[f\"PC{i}\" for i in range(pca.n_components_)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many principal components to retain\n",
    "# Output PCA table\n",
    "pca_result = pd.DataFrame(\n",
    "    {\"Eigenvalue\": pca.explained_variance_,\n",
    "     \"Difference\": np.insert(np.diff(pca.explained_variance_), 0, 0),\n",
    "     \"Proportion\": pca.explained_variance_ratio_,\n",
    "     \"Cumulative\": np.cumsum(pca.explained_variance_ratio_)},\n",
    "    index=range(1, pca.n_components_ + 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure and axes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# draw plots\n",
    "ax1.plot(pca.explained_variance_, marker=\".\", markersize=12)\n",
    "ax2.plot(pca.explained_variance_ratio_, marker=\".\", markersize=12, label=\"Proportion\")\n",
    "ax2.plot(np.cumsum(pca.explained_variance_ratio_), marker=\".\", markersize=12, linestyle=\"--\", label=\"Cumulative\")\n",
    "\n",
    "# customizations\n",
    "ax2.legend()\n",
    "ax1.set_title(\"Scree Plot\", fontsize=14)\n",
    "ax2.set_title(\"Variance Explained\", fontsize=14)\n",
    "ax1.set_ylabel(\"Eigenvalue\")\n",
    "ax2.set_ylabel(\"Proportion\")\n",
    "ax1.set_xlabel(\"Components\")\n",
    "ax2.set_xlabel(\"Components\")\n",
    "\n",
    "ax1.set_xticks(range(0, pca.n_components_, 2))\n",
    "ax1.set_xticklabels(range(1, pca.n_components_ + 1, 2))\n",
    "ax2.set_xticks(range(0, pca.n_components_, 2))\n",
    "ax2.set_xticklabels(range(1, pca.n_components_ + 1, 2))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA again with the number of principal components you want to retain\n",
    "pca = PCA(n_components=1)\n",
    "pca_feat = pca.fit_transform(df_pca[nbh_metric_features])\n",
    "pca_feat_names = [f\"PC{i}\" for i in range(pca.n_components_)]\n",
    "pca_df = pd.DataFrame(pca_feat, index=df_pca.index, columns=pca_feat_names)  # remember index=df_pca.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_data_path = 'computed_data/'\n",
    "\n",
    "if not os.path.exists(computed_data_path): \n",
    "    os.makedirs(computed_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(computed_data_path, 'neighborhood_feat_raw.pickle'), 'wb') as f:\n",
    "    pickle.dump(raw_feat_cluster, f)\n",
    "    \n",
    "with open(os.path.join(computed_data_path, 'neighborhood_PC_cluster.pickle'), 'wb') as f:\n",
    "    pickle.dump(data_cluster_2, f)\n",
    "    \n",
    "with open(os.path.join(computed_data_path, 'neighborhood_feat_after_impute.pickle'), 'wb') as f:\n",
    "    pickle.dump(data_imputed, f)\n",
    "\n",
    "with open(os.path.join(computed_data_path, 'neighborhood_outliers.pickle'), 'wb') as f:\n",
    "    pickle.dump(outliers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(\n",
    "    df_neighbours,\n",
    "    title='Neighbourhood 2',\n",
    "    minimal=True\n",
    ")\n",
    "profile.to_file('explore_nbh_2.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(os.path.join(\"..\", \"data\", \"donors_preprocessed.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
