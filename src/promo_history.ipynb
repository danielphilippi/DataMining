{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random \n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # optionally, you can change 'svg' to 'retina'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitions \n",
    "\n",
    "os.chdir('/Users/dp/Nova/OneDrive - NOVAIMS/1stSemester/DM/DMProject')\n",
    "os.getcwd()\n",
    "\n",
    "computed_data_path = 'computed_data/'\n",
    "explorations_data_path = 'explorations/'\n",
    "\n",
    "paths = [computed_data_path, explorations_data_path]\n",
    "for path in paths:\n",
    "    if not os.path.exists(path): \n",
    "        os.makedirs(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read donors raw\n",
    "if 'donors.pickle' in os.listdir(computed_data_path): \n",
    "    with open(os.path.join(computed_data_path, 'donors.pickle'), 'rb') as f: \n",
    "        donors = pickle.load(f)\n",
    "else: \n",
    "    donors = pd.read_csv('data/donors.csv')\n",
    "    print('donors.head: \\n', donors.head())\n",
    "\n",
    "    with open(os.path.join(computed_data_path, 'donors.pickle'), 'wb') as f: \n",
    "        pickle.dump(donors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all([f in os.listdir('computed_data/') \n",
    "        for f in ['history_feat_raw.pickle', 'cols_with_hist.pickle']\n",
    "       ]):\n",
    "    with open(os.path.join(computed_data_path, 'history_raw.pickle'), 'rb') as f: \n",
    "        history = pickle.load(f)\n",
    "    print(history)\n",
    "\n",
    "    with open(os.path.join(computed_data_path, 'cols_with_hist.pickle'), 'rb') as f: \n",
    "        cols_with_hist = pickle.load(f)\n",
    "    #print(cols_with_hist)\n",
    "else: \n",
    "    # cols with hist: wide to long \n",
    "\n",
    "    patterns = {'adate':'adate_', 'rfa':'rfa_\\d{1,2}$', 'rdate':'RDATE_', 'ramnt':'RAMNT_'}\n",
    "    cols_with_hist = []\n",
    "\n",
    "    history = pd.DataFrame(columns=['CONTROLN', 'promo_no'])\n",
    "\n",
    "    for p_name, pattern_base in patterns.items():\n",
    "\n",
    "        pattern = '|'.join(['CONTROLN', pattern_base])\n",
    "        cols_with_pattern = donors.columns[donors.columns.str.contains(pattern, case=False)]\n",
    "        cols_with_hist.extend(cols_with_pattern)\n",
    "\n",
    "        var_name = p_name+'_no'\n",
    "        pattern_short = re.search('^.*_', pattern_base).group(0)\n",
    "        tmp = pd.melt(donors[cols_with_pattern].copy(deep=True), id_vars='CONTROLN', var_name=var_name, value_name=p_name)\n",
    "        #print(pattern_short)\n",
    "\n",
    "        tmp.loc[tmp[p_name] == ' ', p_name] = np.nan \n",
    "        tmp.dropna(subset=[p_name], inplace=True)\n",
    "\n",
    "\n",
    "        tmp[var_name] = tmp[var_name].str.replace(pattern_short, '', case=False)\n",
    "        tmp['promo_no'] = tmp[var_name]\n",
    "        tmp.drop(columns=[var_name], inplace=True)\n",
    "        #tmp['adate'] = pd.to_datetime(tmp.adate)\n",
    "        #print(tmp)\n",
    "        history = history.merge(tmp, on=['CONTROLN', 'promo_no'], how='outer')\n",
    "        #print(p_name, cols_with_pattern)\n",
    "\n",
    "    cols_with_hist.remove('CONTROLN')\n",
    "\n",
    "\n",
    "    # cast to datetime\n",
    "    history.loc[:, ['adate', 'rdate']] = history.loc[:, ['adate', 'rdate']].apply(pd.to_datetime, format='%Y-%m-%d', errors='raise')\n",
    "    history.dtypes\n",
    "\n",
    "    ## save history to file \n",
    "\n",
    "    with open(os.path.join(computed_data_path, 'history_raw.pickle'), 'wb') as f: \n",
    "        pickle.dump(history, f)\n",
    "\n",
    "    with open(os.path.join(computed_data_path, 'cols_with_hist.pickle'), 'wb') as f: \n",
    "        pickle.dump(cols_with_hist, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history[~(history.adate.isna()) & ~(history.ramnt.isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donors_with_history = history.CONTROLN.nunique() / donors.CONTROLN.nunique()\n",
    "if donors_with_history == 1: \n",
    "    print('All donors have history')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some basic explorations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values\n",
    "print('NAs:\\n',history.isna().sum())\n",
    "\n",
    "\n",
    "# One time donors\n",
    "# Have no rfa...\n",
    "\n",
    "# check one example \n",
    "donors.loc[donors.CONTROLN == 3710,:]\n",
    "\n",
    "# identifie multiple donors\n",
    "donationsPerDoner = history.groupby('CONTROLN')['ramnt'].count() \n",
    "multiple_donors = donationsPerDoner[donationsPerDoner > 1].index.to_list()\n",
    "multiple_donors\n",
    "\n",
    "\n",
    "check = history.loc[(history.rfa.isna()) & (history.CONTROLN.isin(multiple_donors)), :]\n",
    "\n",
    "# identifie multiple donors\n",
    "checkPerDoner = check.groupby('CONTROLN')['ramnt'].count() \n",
    "multiple_donors_check = checkPerDoner[checkPerDoner > 1].index.to_list()\n",
    "multiple_donors_check\n",
    "\n",
    "check = check.loc[history.CONTROLN.isin(multiple_donors_check), :]\n",
    "check = check.sort_values(['CONTROLN'])\n",
    "check.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Inconsistencies and Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Outliers(): \n",
    "    def __init__(self) -> pd.DataFrame:\n",
    "        self.df = pd.DataFrame({\n",
    "            'variable':pd.Series([], dtype='str'),\n",
    "            'rule':pd.Series([], dtype='str'),\n",
    "            'perc_loss_donors':pd.Series([], dtype='float'), \n",
    "            'perc_loss_obs':pd.Series([], dtype='float')\n",
    "        })\n",
    "    \n",
    "    def append(self, var:str, rule:str, perc_loss_donors:float, perc_loss_obs:float):\n",
    "        assert type(var) is str, 'var must be of type str'\n",
    "        assert type(rule) is str, 'rule must be of type str'\n",
    "        assert type(perc_loss_donors) in (float, int), 'diff must be of type float or int'\n",
    "        assert type(perc_loss_obs) in (float, int), 'diff must be of type float or int'\n",
    "\n",
    "        tmp = pd.DataFrame([{\n",
    "            'variable': var,\n",
    "            'rule':rule, \n",
    "            'perc_loss_donors': round(perc_loss_donors, 3), \n",
    "            'perc_loss_obs': round(perc_loss_obs, 3), \n",
    "        }])\n",
    "        self.df = self.df.append(tmp, ignore_index=True)\n",
    "        \n",
    "    def drop(self, positions: list()): \n",
    "        self.df = self.df.drop(positions)\n",
    "        \n",
    "    def drop_all(self): \n",
    "        self.df = self.df.iloc[0:0]\n",
    "        \n",
    "        \n",
    "def def_outliers_iqr(s: pd.Series, factor=1.5, omit_na=False) -> pd.Series: \n",
    "    q25 = s.quantile(.25)\n",
    "    q75 = s.quantile(.75)\n",
    "    iqr = (q75 - q25)\n",
    "\n",
    "    upper_lim = q75 + factor * iqr\n",
    "    lower_lim = q25 - factor * iqr\n",
    "    \n",
    "    if omit_na: \n",
    "        iqr_filter = ((s.between(lower_lim, upper_lim, inclusive=True)))\n",
    "\n",
    "    else: \n",
    "        iqr_filter = ((s.between(lower_lim, upper_lim, inclusive=True)) | (s.isna()))\n",
    "    \n",
    "    return iqr_filter\n",
    "\n",
    "\n",
    "def calc_perc_diff(df, ref): \n",
    "    return(round(((df.shape[0]-ref.shape[0]) / ref.shape[0]), 2))\n",
    "\n",
    "\n",
    "def calc_perc_diff_num(x, ref): \n",
    "    return(round(((x-ref) / ref), 2))\n",
    "\n",
    "def perc_loss_donors(history, filter_def): \n",
    "    return (1-history.loc[filter_def].CONTROLN.nunique() / history.CONTROLN.nunique())*100\n",
    "\n",
    "def perc_loss_obs(history, filter_def): \n",
    "    return (1-history[filter_def].shape[0] / history.shape[0])*100\n",
    "    \n",
    "\n",
    "\n",
    "incons = Outliers()\n",
    "outliers = Outliers()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reply time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check reply times\n",
    "replytime = (history.rdate - history.adate).dt.days\n",
    "p = replytime.value_counts()\n",
    "#sns.histplot(replytime)\n",
    "# sns.histplot(p)\n",
    "\n",
    "print(f'There are {(replytime < 0).sum()} obs with \"adate\" more recent then corresponing \"rdate\".')\n",
    "\n",
    "# remove all doners where reply times are neg\n",
    "replytime_filter_obs = (history.adate <= history.rdate)| history.rdate.isna() | history.adate.isna()\n",
    "CONTROLN_with_rdate_adate_failure = history.loc[~replytime_filter_obs,:].CONTROLN.unique()\n",
    "CONTROLN_with_rdate_adate_failure\n",
    "\n",
    "replytime_filter = ~history.CONTROLN.isin(CONTROLN_with_rdate_adate_failure)\n",
    "\n",
    "incons.append(\n",
    "    var='rdate-adate', \n",
    "    rule='obs where rdate-adate >=0', \n",
    "    perc_loss_donors=perc_loss_donors(history, replytime_filter_obs), \n",
    "    perc_loss_obs=perc_loss_obs(history, replytime_filter_obs)\n",
    ")\n",
    "incons.df\n",
    "\n",
    "incons.append(\n",
    "    var='rdate-adate', \n",
    "    rule='all doners with at least one obs where rdate-adate >=0', \n",
    "    perc_loss_donors=perc_loss_donors(history, replytime_filter), \n",
    "    perc_loss_obs=perc_loss_obs(history, replytime_filter)\n",
    ")\n",
    "incons.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.loc[(replytime_filter) & (history.CONTROLN == 1688)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.loc[(replytime_filter_obs) & (history.CONTROLN == CONTROLN_with_rdate_adate_failure[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply incons filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = history.loc[replytime_filter]\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.dates import DateFormatter\n",
    "fig, ax = plt.subplots(nrows=2, sharey=False) # , figsize=(15,10)\n",
    "\n",
    "sns.histplot(data=history, x='rdate', ax=ax[0])\n",
    "date_form = DateFormatter(\"%Y-%m\")\n",
    "ax[0].xaxis.set_major_formatter(date_form)\n",
    "ax[0].tick_params('x', labelrotation=30)\n",
    "\n",
    "ax[0].title.set_text('Before')\n",
    "\n",
    "\n",
    "# zoom in right tail\n",
    "# sns.histplot(data=history.loc[history.rdate.dt.year == 2016,:], x='rdate')\n",
    "# sns.histplot(data=history.loc[(history.rdate.dt.year == 2016) & (history.rdate.dt.month.isin([2,3,4])),:], x='rdate')\n",
    "\n",
    "# zoom in left tail \n",
    "# sns.histplot(data=history.loc[history.rdate <= '2014-07-01',:], x='rdate')\n",
    "# sns.histplot(data=history.loc[(history.rdate.dt.year == 2014) & (history.rdate.dt.month.isin([5,6,7])),:], x='rdate')\n",
    "\n",
    "\n",
    "\n",
    "sns.histplot(data=history.loc[(history.rdate >= '2014-06-01') & (history.rdate <= '2016-03-01'),:], x='rdate', ax=ax[1])\n",
    "ax[1].title.set_text('After')\n",
    "plt.xticks(rotation=30)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(explorations_data_path, 'outliers_rdate.png'), dpi=200)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# remove rdate outliers and assign to new var 'hist' \n",
    "# remove all obs for respective doner\n",
    "filter_rdate_obs = ((history.rdate.between('2014-06-01', '2016-03-01', inclusive=True)) | (history.rdate.isna()))\n",
    "\n",
    "CONTROLN_with_rdate_outliers = history.loc[~filter_rdate_obs,:].CONTROLN.unique()\n",
    "\n",
    "filter_rdate = ~history.CONTROLN.isin(CONTROLN_with_rdate_outliers)\n",
    "\n",
    "outliers.append(\n",
    "    var='rdate', \n",
    "    rule=\"obs where (history.rdate >= '2014-06-01') & (history.rdate <= '2016-03-01')\",\n",
    "    perc_loss_donors=perc_loss_donors(history, filter_rdate_obs), \n",
    "    perc_loss_obs=perc_loss_obs(history, filter_rdate_obs)\n",
    ")\n",
    "outliers.df\n",
    "\n",
    "outliers.append(\n",
    "    var='rdate', \n",
    "    rule=\"all donors with at least one obs where (history.rdate >= '2014-06-01') & (history.rdate <= '2016-03-01')\",\n",
    "    perc_loss_donors=perc_loss_donors(history, filter_rdate), \n",
    "    perc_loss_obs=perc_loss_obs(history, filter_rdate)\n",
    ")\n",
    "outliers.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history[filter_rdate_obs].rdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history[~filter_rdate_obs].rdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.loc[(filter_rdate_obs) & (history.CONTROLN == 1688)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.loc[(filter_rdate_obs) & (history.CONTROLN == CONTROLN_with_rdate_outliers[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ramnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init plot\n",
    "fig, ax = plt.subplots(nrows=2) # , figsize=(15,10)\n",
    "\n",
    "# plot before\n",
    "sns.histplot(data=history, x = 'ramnt', ax=ax[0])\n",
    "\n",
    "#define filter  \n",
    "filter_ramnt_obs = def_outliers_iqr(history.ramnt, factor=4, omit_na=False)\n",
    "\n",
    "# plot after filter\n",
    "sns.histplot(data=history.loc[filter_ramnt_obs,:], x = 'ramnt', ax=ax[1])\n",
    "\n",
    "ax[0].title.set_text('Before')\n",
    "ax[1].title.set_text('After')\n",
    "#plt.xticks(rotation=30)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(os.path.join(explorations_data_path, 'outliers_ramnt.png'), dpi=200)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#append to oultier overview\n",
    "outliers.append(\n",
    "    var='ramnt', \n",
    "    rule='obs within iqr with factor 4',\n",
    "    perc_loss_donors=perc_loss_donors(history, filter_ramnt_obs), \n",
    "    perc_loss_obs=perc_loss_obs(history, filter_ramnt_obs)\n",
    ")\n",
    "outliers.df\n",
    "\n",
    "\n",
    "CONTROLN_with_ramnt_outliers = history.loc[~filter_ramnt_obs,:].CONTROLN.unique()\n",
    "filter_ramnt = ~history.CONTROLN.isin(CONTROLN_with_ramnt_outliers)\n",
    "\n",
    "#append to oultier overview\n",
    "outliers.append(\n",
    "    var='ramnt', \n",
    "    rule='all doners with at least one obs where iqr with factor 4',\n",
    "    perc_loss_donors=perc_loss_donors(history, filter_ramnt), \n",
    "    perc_loss_obs=perc_loss_obs(history, filter_ramnt)\n",
    ")\n",
    "outliers.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = history[filter_ramnt]\n",
    "sns.histplot(data = tmp, x='ramnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.loc[(filter_ramnt) & (history.CONTROLN == 1688)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.loc[(filter_ramnt_obs) & (history.CONTROLN == CONTROLN_with_ramnt_outliers[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply outlier filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define filters\n",
    "\n",
    "filters_outliers_obs = (\n",
    "    filter_rdate_obs\n",
    "    & \n",
    "    filter_ramnt_obs\n",
    ")\n",
    "\n",
    "# append to outliers overview\n",
    "#append to oultier overview\n",
    "outliers.append(\n",
    "    var='total', \n",
    "    rule='only obs',\n",
    "    perc_loss_donors=perc_loss_donors(history, filters_outliers_obs), \n",
    "    perc_loss_obs=perc_loss_obs(history, filters_outliers_obs)\n",
    ")\n",
    "\n",
    "outliers.df\n",
    "\n",
    "\n",
    "# define filters\n",
    "\n",
    "filters_outliers = (\n",
    "    filter_rdate \n",
    "    & \n",
    "    filter_ramnt\n",
    ")\n",
    "\n",
    "# append to outliers overview\n",
    "#append to oultier overview\n",
    "outliers.append(\n",
    "    var='total', \n",
    "    rule='whole donor with at least one outlier obs',\n",
    "    perc_loss_donors=perc_loss_donors(history, filters_outliers), \n",
    "    perc_loss_obs=perc_loss_obs(history, filters_outliers)\n",
    ")\n",
    "\n",
    "outliers.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incons.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply filters\n",
    "test_history_obs = history.loc[filters_outliers_obs,:] # scenario 'a'\n",
    "test_history_donors = history.loc[filters_outliers,:] # scenario 'b'\n",
    "\n",
    "perc_donors_incons = (1-history.CONTROLN.nunique() /donors.CONTROLN.nunique())*100\n",
    "print(f'Donors lost due to inconsistencies in histroy data: {perc_donors_incons:,.2}%')\n",
    "\n",
    "#perc_donors_ = history[CONTROLN_with_ramnt_outliers].CONTROLN.nunique() /donors.CONTROLN.nunique()\n",
    "#print(f'{perc_donors_nohistory:,.2} % of the donors have no history at all!')\n",
    "\n",
    "print('Additional loss due to outliers in history:')\n",
    "total_perc_filterd_obs = (1-test_history_obs.CONTROLN.nunique() / history.CONTROLN.nunique())*100\n",
    "print(f'- Perc donor loss szenario \"a\" (only dropping outlier obs) : {total_perc_filterd_obs:,.4}%')\n",
    "\n",
    "total_perc_filterd_donors = (1-test_history_donors.CONTROLN.nunique() / history.CONTROLN.nunique())*100\n",
    "print(f'- Perc donor loss szenario \"b\" (dropping whole doner with outliers): {total_perc_filterd_donors:,.4}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donor_id_clean = history.loc[filters_outliers,:].CONTROLN.unique().tolist()\n",
    "donor_id_out1 = history.loc[~filters_outliers,:].CONTROLN.unique().tolist()\n",
    "\n",
    "len(donor_id_clean + donor_id_out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_history_obs.loc[test_history_obs.CONTROLN == CONTROLN_with_ramnt_outliers[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_history_donors.loc[test_history_obs.CONTROLN == CONTROLN_with_ramnt_outliers[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose scenario b: \n",
    "history_out = history.loc[history.CONTROLN.isin(donor_id_out1),:]\n",
    "history = test_history_donors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_out.CONTROLN.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ft(): \n",
    "\n",
    "    def month_diff_series(a, b):\n",
    "        return 12 * (a.dt.year - b.dt.year) + (a.dt.month - b.dt.month)\n",
    "    \n",
    "    def month_diff(a,b): \n",
    "        return 12 * (a.year - b.year) + (a.month - b.month)\n",
    "\n",
    "\n",
    "    def get_freq_days(df:pd.DataFrame, col): \n",
    "        s = (df.loc[:, col].dropna()).sort_values(ascending=False)\n",
    "        #df = df.sort_values(by=col, ascending=False)\n",
    "\n",
    "        res = (s.shift() - s).dropna().dt.days\n",
    "        return(res)\n",
    "\n",
    "    def get_freq_months(df:pd.DataFrame, col): \n",
    "        s = (df.loc[:, col].dropna()).sort_values(ascending=False)\n",
    "        #df = df.sort_values(by=col, ascending=False)\n",
    "\n",
    "        res = ft.month_diff_series(s.shift(), s).dropna()\n",
    "        return(res)\n",
    "\n",
    "    \n",
    "    def get_days_to_last_donation(df, col, max_date): \n",
    "        res = (max_date - df.loc[:,col].max()).days\n",
    "        return(res)\n",
    "\n",
    "    def get_months_to_last_donation(df, col, max_date):\n",
    "        #res = ft.month_diff(pd.Series(max_date), pd.Series(df.loc[:,col].max()))\n",
    "        res = ft.month_diff(max_date, df.loc[:,col].max())\n",
    "        return(res)\n",
    "    \n",
    "    def calc_reply_rate(x, col, ref): \n",
    "        res = (~x.loc[:,col].isna()).sum() / (~x.loc[:,ref].isna()).sum()\n",
    "        res = round(res, 2)\n",
    "        return(res)\n",
    "    \n",
    "    def get_rcy(x, date_col, lim_date): \n",
    "        res = (x[date_col] >= lim_date).sum()\n",
    "        return(res)\n",
    "    \n",
    "    def pct_change(df, val_col, date_col): \n",
    "        # sort values to date\n",
    "        df = df.sort_values(date_col, ascending=False)\n",
    "        # calc percentage change, ignoring na values in between\n",
    "        res = df.loc[~df[val_col].isna(), val_col].pct_change(-1)\n",
    "        if len(res) == 0: \n",
    "            res = pd.Series([np.nan])\n",
    "        return res\n",
    "    \n",
    "    def abs_change(df, val_col, date_col): \n",
    "        # sort values to date\n",
    "        df = df.sort_values(date_col, ascending=False)\n",
    "        # calc percentage change, ignoring na values in between\n",
    "        res = df.loc[~df[val_col].isna(), val_col].diff(-1)\n",
    "        if len(res) == 0: \n",
    "            res = pd.Series([np.nan])\n",
    "        return res\n",
    "\n",
    "#coeff variation: \n",
    "#https://statisticsbyjim.com/basics/coefficient-variation/\n",
    "\n",
    "    def agg_fct(x, max_date): \n",
    "        d = {}\n",
    "        d['donations_total'] = x['ramnt'].sum()\n",
    "        d['donations_mean'] = x['ramnt'].mean()\n",
    "        d['donations_std'] = round(np.std(x['ramnt']), 2)\n",
    "        #print(ft.pct_change(x, 'ramnt', 'rdate'))\n",
    "        d['perc_change_last'] = round(ft.pct_change(x, 'ramnt', 'rdate').iloc[0], 3)\n",
    "        d['abs_change_last'] = round(ft.abs_change(x, 'ramnt', 'rdate').iloc[0], 3)\n",
    "        d['perc_change_mean'] = round(ft.pct_change(x, 'ramnt', 'rdate').mean(), 3)\n",
    "        d['perc_change_sd'] = round(ft.pct_change(x, 'ramnt', 'rdate').std(), 3)\n",
    "        d['n_donations'] = (~x.loc[:,'ramnt'].isna()).sum()\n",
    "        d['freq_mean'] = ft.get_freq_months(x,'rdate').mean()\n",
    "        #print(np.std(ft.get_freq_months(x,'rdate')))\n",
    "        d['freq_std'] = round(np.std(ft.get_freq_months(x,'rdate')), 2)\n",
    "        d['months_to_last_donation'] = ft.get_months_to_last_donation(x, 'rdate', max_date)\n",
    "        d['perc_reply_rate'] =  ft.calc_reply_rate(x, 'rdate', 'adate')\n",
    "        d['mean_reply_time_mean'] = round((x['rdate'] - x['adate']).dt.days.mean(), 2)\n",
    "        d['mean_reply_time_std'] = round(np.std((x['rdate'] - x['adate']).dt.days), 2)\n",
    "        d['rcy'] = ft.get_rcy(x, 'rdate', rcy_lim_date)\n",
    "        return pd.Series(d)\n",
    "\n",
    "max_rdate = history.rdate.max()\n",
    "print('max_rdate', max_rdate)\n",
    "max_date = max_rdate + DateOffset(months=13)\n",
    "print('maxdate', max_date)\n",
    "rcy_lim_date = max_date - DateOffset(months=13+6)\n",
    "print('rcy_lim_date', rcy_lim_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history.set_index(['CONTROLN'], append=True, inplace=True)\n",
    "#history.reset_index(level=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#rdate_freq = history1688.groupby('CONTROLN').rdate.transform(lambda x: (x.shift() - x).dropna().dt.days)\n",
    "\n",
    "\n",
    "def get_freq_series(x:pd.Series): \n",
    "    x = x.sort_values(ascending=False)\n",
    "    \n",
    "    #res = (x.shift() - x).dt.days\n",
    "    res = ft.month_diff_series(x.shift(), x)\n",
    "    return(res)\n",
    "    \n",
    "rdate_freq = history.loc[~history.rdate.isna(),:].groupby('CONTROLN').rdate.transform(lambda x: get_freq_series(x))\n",
    "rdate_freq\n",
    "#(rdate_freq <=0).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rdate_freq_df = rdate_freq.reset_index(level='CONTROLN').dropna()\n",
    "rdate_freq_df.loc[rdate_freq_df.CONTROLN == 1688,:]\n",
    "\n",
    "rdate_freq_clean = rdate_freq.dropna()\n",
    "(rdate_freq_clean<0).sum()\n",
    "(rdate_freq_clean==0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Apply Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### apply feature engineering to doner 1688\n",
    "check_ctrl_no=[1688]\n",
    "history_agg = history.loc[history.CONTROLN.isin(check_ctrl_no),:].groupby('CONTROLN').progress_apply(ft.agg_fct, max_date=max_date)\n",
    "history_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history[history.CONTROLN == 1688]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### apply feature engineering to sample dataset\n",
    "check_ctrl_no = random.choices(population=history.CONTROLN.unique(), k=150)\n",
    "history_agg = history.loc[history.CONTROLN.isin(check_ctrl_no),:].groupby('CONTROLN').progress_apply(ft.agg_fct, max_date=max_date)\n",
    "history_agg\n",
    "#check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### apply feature engineering to complete dataset\n",
    "    \n",
    "if 'history_feat_raw.pickle' in os.listdir(computed_data_path): \n",
    "    with open(os.path.join(computed_data_path, 'history_feat_raw.pickle'), 'rb') as f: \n",
    "        history_agg = pickle.load(f)\n",
    "        \n",
    "else: \n",
    "    history_agg = history.groupby('CONTROLN').progress_apply(ft.agg_fct, max_date=max_date)\n",
    "\n",
    "    with open(os.path.join(computed_data_path, 'history_feat_raw.pickle'), 'wb') as f: \n",
    "        pickle.dump(history_agg, f)\n",
    "\n",
    "history_agg\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### apply feature engineering to outlier dataset\n",
    "    \n",
    "if 'history_feat_raw_out.pickle' in os.listdir(computed_data_path): \n",
    "    with open(os.path.join(computed_data_path, 'history_feat_raw_out.pickle'), 'rb') as f: \n",
    "        history_agg_out = pickle.load(f)\n",
    "        \n",
    "else: \n",
    "    history_agg_out = history_out.groupby('CONTROLN').progress_apply(ft.agg_fct, max_date=max_date)\n",
    "\n",
    "    with open(os.path.join(computed_data_path, 'history_feat_raw_out.pickle'), 'wb') as f: \n",
    "        pickle.dump(history_agg_out, f)\n",
    "\n",
    "    history_agg_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_out.CONTROLN.nunique()\n",
    "history_agg_out\n",
    "history_agg_out.shape[0] / history_agg.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = max(history_agg.columns.get_loc('freq_std'), history_agg.columns.get_loc('freq_mean'))\n",
    "loc\n",
    "history_agg.insert(\n",
    "    loc=loc+1,\n",
    "    column='stability_idx',\n",
    "    value=history_agg.freq_std / history_agg.freq_mean\n",
    ")\n",
    "\n",
    "\n",
    "loc = max(history_agg.columns.get_loc('perc_change_sd'), history_agg.columns.get_loc('perc_change_sd'))\n",
    "loc\n",
    "history_agg.insert(\n",
    "    loc=loc+1,\n",
    "    column='value_stability_idx',\n",
    "    value=history_agg.perc_change_sd / history_agg.perc_change_mean\n",
    ")\n",
    "\n",
    "\n",
    "history_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history[history.CONTROLN == 41]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorations on features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check na in data with at least 2 donations (smaller than that NAs ok to exsist by definition)\n",
    "na_onetime = history_agg.loc[history_agg.n_donations ==1,:].isna().sum()\n",
    "na_twotime = history_agg.loc[history_agg.n_donations ==2,:].isna().sum()\n",
    "na_multiple = history_agg.loc[history_agg.n_donations >=3,:].isna().sum()\n",
    "na_never = history_agg.loc[history_agg.n_donations ==0,:].isna().sum()\n",
    "na_total = history_agg.isna().sum()\n",
    "\n",
    "if False: \n",
    "    print(f'Total NAs:\\n{history_agg.isna().sum()}')\n",
    "    print(f'\\nNAs single:\\n{na_onetime}')\n",
    "    print(f'\\nNAs multi2:\\n{na_twotime}')\n",
    "    print(f'\\nNAs multi>2:\\n{na_multiple}')\n",
    "\n",
    "na_donations = pd.DataFrame(\n",
    "    [na_never, na_onetime, na_twotime, na_multiple],\n",
    "    index=['never', 'onetime', 'twotime', 'multiple']\n",
    ").T\n",
    "\n",
    "if not all(na_donations.sum(axis=1) == na_total): \n",
    "    raise Warning('upps')\n",
    "\n",
    "na_donations.to_excel(os.path.join(explorations_data_path, 'na_hist_feat.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "25057 / history_agg.shape[0]\n",
    "history_agg.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_agg.iloc[977,:]\n",
    "history[history.CONTROLN == 977]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# donors with sd in percentage change is NA\n",
    "# exclusively donors with 3 donations -> by design, but what to do?\n",
    "multidonors_with_percchange_errors = history_agg[(history_agg.perc_change_sd.isna()) & (history_agg.n_donations >=2)].index.to_list()\n",
    "history[history.CONTROLN.isin(multidonors_with_percchange_errors[0:1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donors with percentage change > 1\n",
    "# ok, the more than doubled...\n",
    "donors_high_percchange = history_agg[history_agg.perc_change_mean > 1].index.to_list()\n",
    "history[history.CONTROLN.isin(donors_high_percchange[0:1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# donors with no histroy \n",
    "CONTROLN_nohist = history_agg.loc[history_agg.n_donations == 0,:].index.to_list()\n",
    "print(f'Number of donors with no donation within histroy: {(len(CONTROLN_nohist) / history_agg.shape[0]): .2%}')\n",
    "#donors.loc[donors.CONTROLN.isin(CONTROLN_nohist), cols_with_hist]\n",
    "\n",
    "# one time donors  \n",
    "CONTROLN_onetime = history_agg.loc[history_agg.n_donations == 1,:].index.to_list()\n",
    "print(f'Number of onetime donors: {(len(CONTROLN_onetime) / history_agg.shape[0]): .2%}')\n",
    "#donors.loc[donors.CONTROLN.isin(CONTROLN_onetime), cols_with_hist]\n",
    "\n",
    "CONTROLN_multi = history_agg.loc[history_agg.n_donations >= 2,:].index.to_list()\n",
    "\n",
    "apriory = pd.DataFrame({\n",
    "    'nDonors': [len(CONTROLN_nohist),len(CONTROLN_onetime),len(CONTROLN_multi) ]\n",
    "}, index=['never', 'onetime', 'multiple'])\n",
    "\n",
    "\n",
    "sns.barplot(data=apriory, x=apriory.index, y='nDonors')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare figure\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Obtain correlation matrix. Round the values to 2 decimal cases. Use the DataFrame corr() and round() method.\n",
    "corr = np.round(history_agg.corr(method=\"pearson\"), decimals=2)\n",
    "\n",
    "# Build annotation matrix (values above |0.5| will appear annotated in the plot)\n",
    "mask_annot = np.absolute(corr.values) >= 0.5\n",
    "annot = np.where(mask_annot, corr.values, np.full(corr.shape,\"\")) # Try to understand what this np.where() does\n",
    "\n",
    "# Plot heatmap of the correlation matrix\n",
    "sns.heatmap(data=corr, annot=annot, cmap=sns.diverging_palette(220, 10, as_cmap=True), \n",
    "            fmt='s', vmin=-1, vmax=1, center=0, square=True, linewidths=.5)\n",
    "\n",
    "# Layout\n",
    "fig.subplots_adjust(top=0.95)\n",
    "#fig.suptitle(\"Correlation Matrix\", fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(os.path.join(explorations_data_path, 'correlation_matrix.png'), dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: \n",
    "    # Pairwise Relationship of Numerical Variables\n",
    "    sns.set()\n",
    "\n",
    "    # Setting pairplot\n",
    "    sns.pairplot(history_agg.iloc[:,:], diag_kind=\"none\", corner=False)\n",
    "\n",
    "    # Layout\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    plt.suptitle(\"Pairwise Relationship of Numerical Variables\", fontsize=20)\n",
    "\n",
    "    plt.savefig(os.path.join(explorations_data_path, 'pairwise_relationship_of_numerical_variables.png'), dpi=200)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas\n",
    "- use n donations only to define n=1, n=2 a priori\n",
    "- drop rcy and take month to last donatio instead\n",
    "    - highly corr \n",
    "    - rcy only has few unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_to_drop = ['value_stability_idx', 'rcy', 'perc_change_sd', 'freq_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_palette = sns.color_palette()\n",
    "sns.palplot(current_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_donation_gr(history_agg): \n",
    "    hue_l = ['multiple' if d >=3 else 'twotime' if d >=2 else 'onetime' if d != 0 else 'never' for d in history_agg.n_donations]\n",
    "    hue_order_l = ['never', 'onetime','twotime','multiple']\n",
    "    return hue_l, hue_order_l\n",
    "\n",
    "hue_l, hue_order_l = get_donation_gr(history_agg.drop(columns=feats_to_drop))\n",
    "#palette = dict(zip(hue_order_l, current_palette[0:4]))\n",
    "palette_n_donations = {\n",
    "    'never':current_palette[3] ,\n",
    "    'onetime': current_palette[2],\n",
    "    'twotime': current_palette[1],\n",
    "    'multiple': current_palette[0]\n",
    "} \n",
    "fig = plt.figure()\n",
    "sns.countplot(x = hue_l, palette=palette_n_donations)\n",
    "plt.savefig(os.path.join(explorations_data_path, 'nDonations_grouped.png'), dpi=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(history_agg, palette, name): \n",
    "    ncols = 4\n",
    "    n_plots = history_agg.shape[1]\n",
    "    nrows = int(np.ceil(n_plots/ncols))\n",
    "\n",
    "\n",
    "    if palette is not None:\n",
    "        hue_l, hue_order_l = get_donation_gr(history_agg)\n",
    "    else: \n",
    "        hue_l, hue_order_l = (None, None)\n",
    "\n",
    "    legend = [False]*(n_plots)\n",
    "    legend[0] = True              \n",
    "\n",
    "    fig, ax = plt.subplots(ncols=ncols, nrows=nrows, figsize=(15,10))\n",
    "    col_no = 0\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols): \n",
    "            if col_no < n_plots:\n",
    "                col = history_agg.columns[col_no]\n",
    "                leg = legend[col_no]\n",
    "                print(col)\n",
    "                sns.histplot(\n",
    "                    legend=leg, data=history_agg, x=col,\n",
    "                    hue=hue_l, hue_order=hue_order_l,\n",
    "                    ax=ax[i,j], bins=30,\n",
    "                    palette=palette, alpha=.7\n",
    "                ).set_title(col)\n",
    "                col_no +=1\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.savefig(os.path.join(explorations_data_path, f'distributions_{name}.png'), dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "plot_distribution(history_agg.drop(columns=feats_to_drop), palette=palette_n_donations, name='raw')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_agg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Feature cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stability_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_range(x:list): \n",
    "    if type(x) is list: \n",
    "        x = pd.Series(x)\n",
    "    return(x.min(), x.max())\n",
    "\n",
    "# multidoners with missing stability_idx\n",
    "#print(range(history_agg.stability_idx.values))\n",
    "history_agg[\n",
    "    (history_agg.n_donations > 1) \n",
    "    & (history_agg.stability_idx.isna())\n",
    "    & (history_agg.freq_mean == 0)\n",
    "]\n",
    "\n",
    "\n",
    "print('range_stability_idx: ', get_range(history_agg.stability_idx))\n",
    "\n",
    "filter_multi_stabidx_missing = (\n",
    "    (history_agg.stability_idx.isna())\n",
    "    & (history_agg.n_donations >=2)\n",
    ")\n",
    "    \n",
    "CONTROLN_to_check = history_agg[filter_multi_stabidx_missing].index.to_list()[0:1]\n",
    "history[history.CONTROLN.isin(CONTROLN_to_check)].sort_values(['CONTROLN', 'adate'], ascending=False)\n",
    "\n",
    "# impute\n",
    "history_agg.loc[filter_multi_stabidx_missing, 'stability_idx'] = 0\n",
    "\n",
    "history_agg[history_agg.n_donations >=3].stability_idx.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reply_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi donors but still na in mean reply time...\n",
    "filter_multi_reply_time_na = (\n",
    "    (history_agg.n_donations >=2) \n",
    "    & (history_agg.mean_reply_time_mean.isna())\n",
    ")\n",
    "check_no = history_agg.loc[filter_multi_reply_time_na,:].index.to_list()\n",
    "check_no\n",
    "history.loc[history.CONTROLN.isin(check_no),:]\n",
    "#donors.loc[donors.CONTROLN.isin(check_no),cols_with_hist]\n",
    "\n",
    "# drop case\n",
    "history_agg = history_agg[~filter_multi_reply_time_na]\n",
    "history_agg[history_agg.n_donations >=3].mean_reply_time_mean.isna().sum()\n",
    "\n",
    "history_agg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers removal on features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True: \n",
    "    feats_to_drop += [\n",
    "        'n_donations'\n",
    "    ]\n",
    "\n",
    "feats_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_feats = history_agg.drop(columns=feats_to_drop).columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_multi = history_agg.n_donations >=2\n",
    "history_agg_multi = history_agg[filter_multi].drop(columns=feats_to_drop)\n",
    "history_agg_multi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_outlier_manual(df, col, th): \n",
    "    \n",
    "\n",
    "    if th is None: \n",
    "        th = df[col].max()\n",
    "    filter_col = df[col] <= th\n",
    "    sns.histplot(data=df[filter_col], x=col, bins=30)\n",
    "\n",
    "    perc_loss = round((1 - filter_col.sum() / df.shape[0])*100, 2)\n",
    "    print(f'perc_loss: {perc_loss}%')\n",
    "    plt.show()\n",
    "\n",
    "    return col, th, perc_loss, filter_col\n",
    "\n",
    "outliers_manual = {}\n",
    "filter_out_man = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col, th, perc_loss, filter_col = rm_outlier_manual(\n",
    "    df=history_agg_multi,\n",
    "    col='perc_change_last',\n",
    "    th=3\n",
    ")\n",
    "\n",
    "outliers_manual[col] = (th, perc_loss)\n",
    "filter_out_man[col] = filter_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col, th, perc_loss, filter_col = rm_outlier_manual(\n",
    "    df=history_agg_multi,\n",
    "    col='perc_change_mean',\n",
    "    th=3\n",
    ")\n",
    "\n",
    "outliers_manual[col] = (th, perc_loss)\n",
    "filter_out_man[col] = filter_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col, th, perc_loss, filter_col = rm_outlier_manual(\n",
    "    df=history_agg_multi,\n",
    "    col='mean_reply_time_std',\n",
    "    th=120\n",
    ")\n",
    "\n",
    "outliers_manual[col] = (th, perc_loss)\n",
    "filter_out_man[col] = filter_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col, th, perc_loss, filter_col = rm_outlier_manual(\n",
    "    df=history_agg_multi,\n",
    "    col='stability_idx',\n",
    "    th=1.3\n",
    ")\n",
    "\n",
    "outliers_manual[col] = (th, perc_loss)\n",
    "filter_out_man[col] = filter_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'n_donations' in history_agg_multi.columns.to_list():\n",
    "    col, th, perc_loss, filter_col = rm_outlier_manual(\n",
    "        df=history_agg_multi,\n",
    "        col='n_donations',\n",
    "        th=12\n",
    "    )\n",
    "\n",
    "    outliers_manual[col] = (th, perc_loss)\n",
    "    filter_out_man[col] = filter_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df_man = pd.DataFrame(outliers_manual, index=['value', 'perc_loss']).T\n",
    "loss_df_man.insert(\n",
    "    loc=0, \n",
    "    column='rule', \n",
    "    value='manual'\n",
    ")\n",
    "\n",
    "loss_df_man\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all manual filters\n",
    "\n",
    "filter_out_man_df = pd.DataFrame(filter_out_man)\n",
    "filter_out_man_df\n",
    "\n",
    "filter_out_man_comb = np.all(filter_out_man_df, 1)\n",
    "filter_out_man_comb\n",
    "\n",
    "# \n",
    "feats_out_man = filter_out_man_df.columns.to_list()\n",
    "feats_out_man \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_outliers_init = []\n",
    "perc_loss_init = []\n",
    "for col in history_agg_multi.drop(columns=feats_out_man).columns: \n",
    "    \n",
    "    filter_tmp = def_outliers_iqr(history_agg_multi[col], factor=1.5, omit_na=False)\n",
    "    filter_outliers_init.append(filter_tmp)\n",
    "    \n",
    "    loss = (1-(filter_tmp.sum() / history_agg_multi.shape[0]))*100\n",
    "    perc_loss_init.append((col, loss))\n",
    "    if False: \n",
    "        print(col)\n",
    "        fig, ax = plt.subplots(ncols=2)\n",
    "        sns.histplot(data=history_agg_multi, x = col, bins=30, ax=ax[0])\n",
    "        sns.histplot(data=history_agg_multi[filter_tmp], x = col, bins=30, ax=ax[1])\n",
    "        plt.show()\n",
    "\n",
    "perc_loss_init\n",
    "#filter_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame(perc_loss_init, columns=['col', 'perc_loss_init']).sort_values('perc_loss_init')\n",
    "loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df['factor'] = [1.5 if l<1 else 2.5 if l<5 else 5 if l<10 else 6.5 for l in loss_df.perc_loss_init.values]\n",
    "if 'col' in loss_df.columns: \n",
    "    loss_df.set_index('col', inplace=True)\n",
    "    \n",
    "loss_df.loc['donations_mean','factor'] = 3.5\n",
    "#loss_df.loc['n_donations','factor'] = 3.5\n",
    "#loss_df.loc['perc_change_last','factor'] = 10\n",
    "#loss_df.loc['perc_change_mean','factor'] = 10\n",
    "#loss_df.loc['mean_reply_time_std','factor'] = 7.5\n",
    "\n",
    "loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filter_outliers = []\n",
    "perc_loss = []\n",
    "for col in history_agg_multi.drop(columns=feats_out_man).columns: \n",
    "    fac = loss_df.loc[col,:].factor\n",
    "    print(col, fac)\n",
    "\n",
    "    \n",
    "    filter_tmp = def_outliers_iqr(history_agg_multi[col], factor=fac, omit_na=False)\n",
    "    filter_outliers.append(filter_tmp)\n",
    "    \n",
    "    loss = round((1-(filter_tmp.sum() / history_agg_multi.shape[0]))*100, 4)\n",
    "    perc_loss.append((col, loss))\n",
    "    if False: \n",
    "        fig, ax = plt.subplots(ncols=2)\n",
    "        sns.histplot(data=history_agg_multi, x = col, bins=30, ax=ax[0])\n",
    "        sns.histplot(data=history_agg_multi[filter_tmp], x = col, bins=30, ax=ax[1])\n",
    "        plt.show()\n",
    "\n",
    "perc_loss\n",
    "#filter_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(perc_loss, columns=['col', 'perc_loss']).sort_values('perc_loss')\n",
    "tmp.set_index('col', inplace=True)\n",
    "loss_df = pd.concat([loss_df, tmp], axis=1)#.sort_values('perc_loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters2 = pd.Series(np.all(filter_outliers, 0), index=history_agg_multi.index)\n",
    "\n",
    "filters2_comb = (\n",
    "    filters2\n",
    "    & filter_out_man_comb\n",
    ")\n",
    "filters2_comb\n",
    "\n",
    "history_agg_multi_clean = history_agg_multi[filters2_comb].copy()\n",
    "\n",
    "perc_loss_outliers2 = (1- history_agg_multi_clean.shape[0] / history_agg_multi.shape[0])*100\n",
    "print(f'perc_loss_outliers2: {perc_loss_outliers2:.4}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df.sort_values('factor', inplace=True)\n",
    "loss_df['perc_loss_init'] = round(loss_df['perc_loss_init'],4)\n",
    "\n",
    "loss_df_iqr = loss_df.copy()\n",
    "loss_df_iqr.rename(columns={'factor': 'value'}, inplace=True)\n",
    "loss_df_iqr.insert(\n",
    "    loc=1,\n",
    "    column='rule', \n",
    "    value='iqr - factor'\n",
    ")\n",
    "\n",
    "loss_df_comb = pd.concat([loss_df_man, loss_df_iqr])\n",
    "loss_df_comb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_df_comb.loc['sum'] = np.nan\n",
    "loss_df_comb.loc['sum', 'perc_loss'] = round(loss_df_comb.perc_loss.sum(), 2)\n",
    "\n",
    "loss_df_comb.loc['overall'] = np.nan\n",
    "loss_df_comb.loc['overall', 'perc_loss'] = round(perc_loss_outliers2, 2)\n",
    "\n",
    "\n",
    "loss_df_comb.to_excel(os.path.join(explorations_data_path, 'outliers2.xlsx'))\n",
    "\n",
    "loss_df_comb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(history_agg_multi_clean, palette=None, name='clean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features\n",
    "history_agg_multi_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters2_comb[filters2_comb == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_agg_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donor_id_out2 = filters2_comb[filters2_comb == False].index.to_list()\n",
    "donor_id_out2\n",
    "\n",
    "history_agg_out2 = history_agg[history_agg.index.isin(donor_id_out2)]\n",
    "\n",
    "history_agg_out_total = pd.concat([history_agg_out, history_agg_out2])\n",
    "history_agg_out_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_agg_out_total.shape[0] / history_agg.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_agg_out_total.n_donations.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_agg_out.n_donations.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_agg_out_multi = history_agg_out.loc[history_agg_out.n_donations >=2,:]\n",
    "history_agg_out_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "\n",
    "with open(os.path.join(computed_data_path, 'history_feat_multi_out.pickle'), 'wb') as f: \n",
    "    pickle.dump(history_agg_out_multi, f)\n",
    "    \n",
    "with open(os.path.join(computed_data_path, 'history_feat_multi_clean.pickle'), 'wb') as f: \n",
    "    pickle.dump(history_agg_multi_clean, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature profile report\n",
    "profile = ProfileReport(\n",
    "    history_agg, \n",
    "    title='Donors Hist Agg1',\n",
    "    minimal=False, \n",
    "    correlations={\n",
    "    \"pearson\": {\"calculate\": True},\n",
    "    \"spearman\": {\"calculate\": False},\n",
    "    \"kendall\": {\"calculate\": False},\n",
    "    \"phi_k\": {\"calculate\": False},\n",
    "    \"cramers\": {\"calculate\": False},\n",
    "    }\n",
    ")\n",
    "profile.to_file('explorations/profile_hist_agg1_v2.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "profile = ProfileReport(\n",
    "    history_agg_2,\n",
    "    title='Donors Hist Agg 2',\n",
    "    minimal=False, \n",
    "    correlations={\n",
    "    \"pearson\": {\"calculate\": True},\n",
    "    \"spearman\": {\"calculate\": False},\n",
    "    \"kendall\": {\"calculate\": False},\n",
    "    \"phi_k\": {\"calculate\": False},\n",
    "    \"cramers\": {\"calculate\": False},\n",
    "    }\n",
    ")\n",
    "profile.to_file('explorations/profile_hist_agg_2.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_histsumm = [\n",
    "    'RAMNTALL',\n",
    "    'NGIFTALL',\n",
    "    'CARDGIFT',\n",
    "    'MINRAMNT',\n",
    "    'MINRDATE',\n",
    "    'MAXRAMNT',\n",
    "    'MAXRDATE',\n",
    "    'LASTGIFT',\n",
    "    'LASTDATE',\n",
    "    'FISTDATE',\n",
    "    'NEXTDATE',\n",
    "    'TIMELAG',\n",
    "    'AVGGIFT'\n",
    "]\n",
    "\n",
    "donors.loc[:,cols_with_histsumm + ['CONTROLN']].set_index('CONTROLN', append=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "profile = ProfileReport(\n",
    "    donors.loc[:,cols_with_histsumm], \n",
    "    title='Donors Hist Summary',\n",
    "    minimal=False, \n",
    "    correlations={\n",
    "    \"pearson\": {\"calculate\": True},\n",
    "    \"spearman\": {\"calculate\": False},\n",
    "    \"kendall\": {\"calculate\": False},\n",
    "    \"phi_k\": {\"calculate\": False},\n",
    "    \"cramers\": {\"calculate\": False},\n",
    "    }\n",
    ")\n",
    "profile.to_file('explorations/profile_hist_summ.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
