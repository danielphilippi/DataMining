{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.preprocessing as prep\n",
    "from datetime import date, timedelta , datetime\n",
    "import math\n",
    "import os \n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder \n",
    "import pickle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_data_path = '../computed_data/'\n",
    "\n",
    "if not os.path.exists(computed_data_path): \n",
    "    os.makedirs(computed_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original  = pd.read_csv('../data/donors.csv')\n",
    "data = data_original.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.CONTROLN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.set_index('CONTROLN', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#description = ['ODATEDW' , 'OSOURCE' , 'TCODE' , 'STATE' , 'ZIP' , 'MAILCODE' , 'PVASTATE' , 'DOB' , 'NOEXCH' , 'RECINHSE' , 'RECP3' , 'RECPGVG' , 'RECSWEEP','MDMAUD' , 'DOMAIN' , 'HOMEOWNR' , 'CHILD03' , 'CHILD07' , 'CHILD12' , 'CHILD18' , 'NUMCHLD' , 'INCOME' , 'GENDER' , 'WEALTH1' , 'HIT' ,'MBCRAFT' , 'MBGARDEN' , 'MBBOOKS' , 'MBCOLECT' , 'MAGFAML' , 'MAGFEM' , 'MAGMALE' , 'PUBGARDN' , 'PUBCULIN' , 'PUBHLTH' , 'PUBDOITY' ,'PUBNEWFN' , 'PUBPHOTO', 'PUBOPP' ,'DATASRCE' , 'MALEMILI' , 'MALEVET' , 'VIETVETS' , 'WWIIVETS' , 'LOCALGOV' , 'STATEGOV' , 'FEDGOV' , 'SOLP3' ,'SOLIH' , 'MAJOR' , 'WEALTH2' , 'GEOCODE' , 'COLLECT1' , 'VETERANS' , 'BIBLE', 'CATLG' , 'HOMEE' , 'PETS', 'CDPLAY' , 'STEREO' , 'PCOWNERS' , 'PHOTO' , 'CRAFTS' , 'FISHER' , 'GARDENIN' , 'BOATS' , 'WALKER' , 'KIDSTUFF' , 'CARDS' , 'PLATES' , 'LIFESRC' , 'PEPSTRFL']\n",
    "numeric_features = ['NUMCHLD','INCOME' , 'WEALTH1' , 'MBCRAFT' , 'MBGARDEN' , 'MBBOOKS' , 'MBCOLECT' ,\n",
    "                    'MAGFAML' , 'MAGFEM' , 'MAGMALE' , 'PUBGARDN' , 'PUBCULIN' , 'PUBHLTH' , 'PUBDOITY' , 'PUBNEWFN' , 'PUBOPP' , 'WEALTH2' ]\n",
    "boolean_features = [ 'PEPSTRFL', 'MAILCODE', 'PVASTATE', 'RECINHSE', 'RECP3', 'RECPGVG', 'RECSWEEP', 'MAJOR']\n",
    "donors_interests = ['COLLECT1','VETERANS' , 'BIBLE' , 'CATLG' , 'HOMEE' , 'PETS' , 'CDPLAY','STEREO' , 'PCOWNERS' , 'PHOTO' , 'CRAFTS' , 'FISHER' , 'GARDENIN' , 'BOATS' , 'WALKER' , 'KIDSTUFF' , 'CARDS'  , 'PLATES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [ '#4C72B0' , '#dd8452' , '#55A868' , '#C44E52']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of missing values in each feature\n",
    "sns.set()\n",
    "\n",
    "#figure and axis\n",
    "fig, (ax1) = plt.subplots(1, figsize=(20,6))\n",
    "\n",
    "#data\n",
    "ax1_data=data[numeric_features].isna().sum().sort_values(ascending=False)\n",
    "ax1.bar(x=ax1_data.index, height=ax1_data)\n",
    "ax1.set_title(\"Missing Values per Variable\", fontsize=20)\n",
    "ax1.set_ylabel(\"Total Missing Values\")\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "\n",
    "for tick in ax1.get_xticklabels():\n",
    "    tick.set_rotation(-55)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECK INCOHERENCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DONATION DATES BEFORE DONOR WAS BORN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[( pd.to_datetime(data.DOB) ) > ( pd.to_datetime(data.ODATEDW) )][['DOB','ODATEDW']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DONOR WITH LESS THAN 14 (MINIMUM AGE TO WORK IN THE US) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[ (pd.to_datetime(data.DOB).dt.year > 2006 )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DONORS WITH LESS THAN 14 WHICH ARE HOMEOWNERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[ (pd.to_datetime(data.DOB).dt.year > 2006 ) & (data.HOMEOWNR == 'H' ) ][['DOB','HOMEOWNR']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DONORS WITH LESS THAN 14 WHICH RECEIVE AN INCOME (14 IS THE MINIMUM AGE TO WORK IN THE US )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[ (pd.to_datetime(data.DOB).dt.year > 2006 ) & (data.INCOME > 1 ) ][['DOB','INCOME']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECK FOR SITUATIONS WHERE THE NUMBER OF CHILDRENS DECLARED ON COLUMNS 'CHILD03' , 'CHILD07' , 'CHILD12' AND 'CHILD18' IS BIGGER THAN THE CHILDREN DECLARED ON NUMCHLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkChildrenInconsistencies(x):  \n",
    "    numberOfChildren = 0\n",
    "    colsToCheck = ['CHILD03' , 'CHILD07' , 'CHILD12' , 'CHILD18' ]\n",
    "    for i in colsToCheck:\n",
    "        if (x[i] == 'B'):\n",
    "            numberOfChildren = numberOfChildren + 2\n",
    "        elif (x[i] == 'M' or x[i] == 'F'):\n",
    "            numberOfChildren = numberOfChildren + 1\n",
    "            \n",
    "    return numberOfChildren\n",
    "  \n",
    "    \n",
    "data['MINIMUM_POSSIBLE_CHILD'] = data.apply(lambda x: checkChildrenInconsistencies(x) , axis=1)\n",
    "data.loc[ data.MINIMUM_POSSIBLE_CHILD > data.NUMCHLD][['CHILD03' , 'CHILD07' , 'CHILD12' , 'CHILD18', 'NUMCHLD' , 'MINIMUM_POSSIBLE_CHILD'  ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['MINIMUM_POSSIBLE_CHILD']\n",
    "numeric_features.remove('NUMCHLD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOEXCH values not documented , only possible values are \" \" and \"X\"\n",
    "data.NOEXCH = data.NOEXCH.astype(str)\n",
    "\n",
    "sns.set()\n",
    "sns.countplot(x=\"NOEXCH\", data=data)\n",
    "plt.suptitle(\"NOEXCH Frequencies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CARDINALITY REDUCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENDER\n",
    "plt.title(\"Gender Distribution\")\n",
    "sns.countplot(x = 'GENDER',\n",
    "              data = data , order = data.GENDER.value_counts().index )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.GENDER = data.GENDER.apply(lambda x: 'M' if x == 'M' else 'F' if x == 'F' else 'O' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AS COLUMN DOMAIN HAS SPECIFIC VALUE FOR EACH CHARACTER, WE WILL SPLIT IN TWO COLUMNS AND RESCALE THE SECOND INTO A UNIFORM SCALE WITHOUT THE PECULIARITIES FOR THE URBAN COMMUNITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOMAIN\n",
    "data['DOMAIN_1'] = data.DOMAIN.astype(str).str[0]\n",
    "data['DOMAIN_2'] = data.DOMAIN.astype(str).str[1]\n",
    "\n",
    "data['DOMAIN_2'] = ['5' if b == '4' or (b == '3' and a != 'U' ) else b for a,b in zip(data['DOMAIN_1'] , data['DOMAIN_2'] ) ]\n",
    "data['DOMAIN_2'] = ['4' if a == 'U' and b == '3' else b for a, b in zip(data['DOMAIN_1'] , data['DOMAIN_2'] ) ]\n",
    "data['DOMAIN_2'] = ['3' if b == '2' and a != 'U' else b for a, b in zip(data['DOMAIN_1'] , data['DOMAIN_2'] ) ]\n",
    "\n",
    "data['DOMAIN_2'] = pd.to_numeric(data['DOMAIN_2'], errors='coerce')\n",
    "numeric_features.append('DOMAIN_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE COLUMN AGE BASED ON 'DOB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We assumed the current date as 2020/12/31\n",
    "import datetime\n",
    "\n",
    "x = datetime.date(2020, 12, 31)\n",
    "\n",
    "def findAge(dob ):\n",
    "    if str(dob) == \"NaT\" :\n",
    "        return np.nan\n",
    "    else :\n",
    "        age = (x - dob.date() ) // timedelta(days=365.2425)\n",
    "        return age    \n",
    "    \n",
    "    \n",
    "\n",
    "data['DOB'] = pd.to_datetime(data.DOB)\n",
    "data['AGE'] = data.apply(lambda x: findAge(x.DOB ), axis=1)\n",
    "numeric_features.append('AGE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE COLUMN YEARS_SINCE_FIRST_DONATION BASED ON 'DATEDW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def yearsSinceFirstDonor(odatedw):\n",
    "    date_transform = datetime.strptime(odatedw, '%Y-%m-%d').date()\n",
    "    years = (x - date_transform ) // timedelta(days=365.2425)\n",
    "    return years   \n",
    "    \n",
    "    \n",
    "data['YEARS_SINCE_FIRST_DONATION'] = data.apply(lambda x: yearsSinceFirstDonor(x.ODATEDW), axis=1)\n",
    "numeric_features.append('YEARS_SINCE_FIRST_DONATION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"YEARS_SINCE_FIRST_DONATION Distribution\")\n",
    "sns.countplot(x = 'YEARS_SINCE_FIRST_DONATION',\n",
    "              data = data , order = data.YEARS_SINCE_FIRST_DONATION.value_counts().index )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BINARY TRANSFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in donors_interests:\n",
    "    data[col_name] = data[col_name].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "    data[col_name] = data[col_name].astype(str).astype(int)\n",
    "    \n",
    "data.PVASTATE = data.PVASTATE.apply(lambda x: 1 if x == 'P' else 1 if x == 'E' else 0 )\n",
    "\n",
    "x_or_u = ['RECINHSE', 'RECP3' , 'RECPGVG' , 'RECSWEEP' , 'MAJOR' , 'PEPSTRFL' ]\n",
    "for i in x_or_u:\n",
    "    data[i] = data[i].apply(lambda x: 1 if x == 'X' else 0 )\n",
    "    \n",
    "data.MAILCODE = data.MAILCODE.apply(lambda x: 1 if x == 'B' else 0 )\n",
    "data.PVASTATE = data.PVASTATE.apply(lambda x: 1 if x == 'P' else 1 if x == 'E' else 0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHANGING EMPTY CLASSES (' ') VALUES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.LIFESRC = data.LIFESRC.apply(lambda x: '1' if x == '1' else '2' if x == '2' else '3' if x == '3' else '0' )\n",
    "data.DATASRCE = data.DATASRCE.apply(lambda x: '1' if x == '1' else '2' if x == '2' else '3' if x == '3' else '0' )\n",
    "data.SOLIH = data.SOLIH.apply(lambda x: '13' if x == ' ' else x )\n",
    "data.SOLIH = data.SOLIH.astype(str).astype(int)\n",
    "numeric_features.append('SOLIH')\n",
    "data.SOLP3 = data.SOLP3.apply(lambda x: '13' if x == ' ' else x )\n",
    "data.SOLP3 = data.SOLP3.astype(str).astype(int)\n",
    "numeric_features.append('SOLP3')\n",
    "data.HOMEOWNR = data.HOMEOWNR.apply(lambda x: 'H' if x == 'H' else 'U' if x == 'U' else 'O' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ONE HOT ENCODE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ohc = data.copy()\n",
    "to_encode = [ 'GENDER'  , 'DOMAIN_1' , 'DATASRCE' , 'LIFESRC' , 'HOMEOWNR']\n",
    "\n",
    "ohc = OneHotEncoder(sparse=False, drop=\"first\")\n",
    "ohc_feat = ohc.fit_transform(data_ohc[to_encode])\n",
    "ohc_feat_names = ohc.get_feature_names()\n",
    "ohc_df = pd.DataFrame(ohc_feat, index=data_ohc.index, columns=ohc_feat_names) \n",
    "ohc_df\n",
    "\n",
    "#GENDER ->  x0_M ; x0_O\n",
    "#DOMAIN_1 -> x1_C ; x1_R ; x1_S ;  x1_T ; x1_U\n",
    "#DATASRCE -> x2_1 ; x2_2 ; x2_3\n",
    "#LIFESRC -> x3_1 ; x3_2 ; x3_3\n",
    "#HOMEOWNR -> x4_O ; x4_U\n",
    "\n",
    "data_ohc = pd.concat([data_ohc.drop(columns=to_encode), ohc_df], axis=1)\n",
    "data = data_ohc\n",
    "\n",
    "data = data.rename(columns={'x0_M':'GENDER_M', 'x0_O':'GENDER_O', 'x1_C':'DOMAIN_1_C', 'x1_R':'DOMAIN_1_R', 'x1_S':'DOMAIN_1_S' , 'x1_T':'DOMAIN_1_T', 'x1_U':'DOMAIN_1_U', 'x2_1':'DATASRCE_1',\n",
    "                  'x2_2':'DATASRCE_2', 'x2_3':'DATASRCE_3', 'x3_1':'LIFESRC_1', 'x3_2':'LIFESRC_2', 'x3_3':'LIFESRC_3' , 'x4_O':'HOMEOWNR_O', 'x4_U':'HOMEOWNR_U'})\n",
    "\n",
    "\n",
    "new_col_names = ['GENDER_M' , 'GENDER_O' , 'DOMAIN_1_C' , 'DOMAIN_1_R' , 'DOMAIN_1_S', 'DOMAIN_1_T' , 'DOMAIN_1_U' , 'DATASRCE_1', 'DATASRCE_2' , 'DATASRCE_3' , 'LIFESRC_1', 'LIFESRC_2','LIFESRC_3' , 'HOMEOWNR_O' , 'HOMEOWNR_U']\n",
    "\n",
    "boolean_features = [ 'PEPSTRFL', 'MAILCODE', 'PVASTATE', 'RECINHSE', 'RECP3', 'RECPGVG', 'RECSWEEP', 'MAJOR']\n",
    "\n",
    "\n",
    "description = numeric_features + new_col_names + boolean_features + donors_interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['STATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(computed_data_path, 'description_features.pickle'), 'wb') as f:\n",
    "       pickle.dump(data[description], f)\n",
    "\n",
    "data = pd.merge(data[['STATE' , 'ZIP']] , data[description] , left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_profiles(df, label_columns, figsize, compar_titles=None):\n",
    "    \"\"\"\n",
    "    Pass df with labels columns of one or multiple clustering labels. \n",
    "    Then specify this label columns to perform the cluster profile according to them.\n",
    "    \"\"\"\n",
    "    if compar_titles == None:\n",
    "        compar_titles = [\"\"]*len(label_columns)\n",
    "        \n",
    "    sns.set()\n",
    "    fig, axes = plt.subplots(nrows=len(label_columns), ncols=2, figsize=figsize, squeeze=False)\n",
    "    for ax, label, titl in zip(axes, label_columns, compar_titles):\n",
    "        # Filtering df\n",
    "        drop_cols = [i for i in label_columns if i!=label]\n",
    "        dfax = df.drop(drop_cols, axis=1)\n",
    "        \n",
    "        # Getting the cluster centroids and counts\n",
    "        centroids = dfax.groupby(by=label, as_index=False).mean()\n",
    "        counts = dfax.groupby(by=label, as_index=False).count().iloc[:,[0,1]]\n",
    "        counts.columns = [label, \"counts\"]\n",
    "        \n",
    "        # Setting Data\n",
    "        pd.plotting.parallel_coordinates(centroids, label, color=sns.color_palette(), ax=ax[0])\n",
    "        sns.barplot(x=label, y=\"counts\", data=counts, ax=ax[1])\n",
    "\n",
    "        #Setting Layout\n",
    "        handles, _ = ax[0].get_legend_handles_labels()\n",
    "        cluster_labels = [\"Cluster {}\".format(i) for i in range(len(handles))]\n",
    "        #print('cluster_labels: ', cluster_labels)\n",
    "        \n",
    "        color_mapper = {}\n",
    "        for handle in handles: \n",
    "            color_mapper[handle.get_label()] = handle.get_color()\n",
    "        \n",
    "        #print('handles: ', handles[0].get_color())\n",
    "        #print('handles: ', handles[0].get_label())\n",
    "\n",
    "\n",
    "        ax[0].annotate(text=titl, xy=(0.95,1.1), xycoords='axes fraction', fontsize=13, fontweight = 'heavy') \n",
    "        ax[0].legend(handles, cluster_labels) # Adaptable to number of clusters\n",
    "        ax[0].axhline(color=\"black\", linestyle=\"--\")\n",
    "        ax[0].set_title(\"Cluster Means - {} Clusters\".format(len(handles)), fontsize=13)\n",
    "        ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=30)\n",
    "        ax[1].set_xticklabels(cluster_labels)\n",
    "        ax[1].set_xlabel(\"\")\n",
    "        ax[1].set_ylabel(\"Absolute Frequency\")\n",
    "        ax[1].set_title(\"Cluster Sizes - {} Clusters\".format(len(handles)), fontsize=13)\n",
    "    \n",
    "    #plt.subplots_adjust(hspace=0.4, top=0.90)\n",
    "    plt.tight_layout()\n",
    "    plt.title = ''\n",
    "#    plt.suptitle(\"Cluster Simple Profilling\", fontsize=23)\n",
    "\n",
    "    filename = ''.join(label_columns)\n",
    "    plt.savefig(os.path.join(f'profile_mean_{filename}.jpeg'), dpi=200)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return color_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[donors_interests].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters  = pd.read_csv('computed_data/cluster_donor_mapping.csv' )\n",
    "clusters.set_index('CONTROLN', inplace=True)\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_merged = pd.merge(data, clusters, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index.intersection(clusters.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(clusters_merged[donors_interests])\n",
    "scaled_data = scaler.transform(clusters_merged[donors_interests])\n",
    "\n",
    "clusters_merged[donors_interests] = scaled_data\n",
    "\n",
    "# Profilling each cluster (product, behavior, merged)\n",
    "cluster_profiles(\n",
    "    df = clusters_merged[ donors_interests + ['merged_labels']], \n",
    "    label_columns = ['merged_labels'], \n",
    "    figsize = (28, 13), \n",
    "    compar_titles = [\"Donor interests clustering\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_details = ['GENDER_M', 'GENDER_O', 'DOMAIN_1_C' , 'DOMAIN_1_R' , 'DOMAIN_1_S', 'DOMAIN_1_T' , 'DOMAIN_1_U', 'DOMAIN_2' , 'HOMEOWNR_O' , 'HOMEOWNR_U' , 'INCOME' , 'AGE' , 'YEARS_SINCE_FIRST_DONATION']\n",
    "\n",
    "secondScaler = MinMaxScaler()\n",
    "secondScaler.fit(clusters_merged[personal_details])\n",
    "scaled_data_2 = secondScaler.transform(clusters_merged[personal_details])\n",
    "\n",
    "clusters_merged[personal_details] = scaled_data_2\n",
    "\n",
    "# Profilling each cluster (product, behavior, merged)\n",
    "cluster_profiles(\n",
    "    df = clusters_merged[personal_details + ['merged_labels']], \n",
    "    label_columns = ['merged_labels'], \n",
    "    figsize = (28, 13), \n",
    "    compar_titles = [\"Personal Details clustering\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_merged[['STATE','merged_labels','neigh_labels', 'hist_labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo = pd.DataFrame(clusters_merged.groupby(['STATE','merged_labels']).size().to_frame('SIZE') )\n",
    "geo = geo.reset_index()\n",
    "\n",
    "states  = pd.read_csv('data/usa_states.csv' , sep=';')\n",
    "\n",
    "states.head()\n",
    "geographic_info = pd.merge(geo, states, left_on='STATE', right_on='state')\n",
    "geographic_info = geographic_info.drop(columns=['state'])\n",
    "geographic_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "limits = geographic_info.merged_labels.unique().tolist()\n",
    "\n",
    "lat = [0 , 0 , 0.7 , -0.7]\n",
    "lon = [0.5 , -0.5 , 0 , 0]\n",
    "cities = []\n",
    "scale = 4\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in limits:    \n",
    "    lim = limits[i]\n",
    "    df_sub = geographic_info.loc[(geographic_info.merged_labels == lim)]\n",
    "    fig.add_trace(go.Scattergeo(\n",
    "        locationmode = 'USA-states',\n",
    "        lon = df_sub['longitude'] + lon[i],\n",
    "        lat = df_sub['latitude'] + lat[i],\n",
    "        text = df_sub[[ \"name\", 'SIZE']],\n",
    "        marker = dict(\n",
    "            size = df_sub['SIZE']/scale,\n",
    "            color = colors[i],\n",
    "            line_color='rgb(40,40,40)',\n",
    "            line_width=0.5,\n",
    "            sizemode = 'area'\n",
    "        ),\n",
    "        name = 'Cluster - {0} '.format(lim)))\n",
    "        \n",
    "fig.update_layout(\n",
    "        title_text = 'Cluster Distribution per State',\n",
    "        showlegend = True,\n",
    "        geo = dict(\n",
    "            scope = 'usa',\n",
    "            landcolor = 'rgb(217, 217, 217)',\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = geographic_info.groupby(['STATE'])['SIZE'].transform(max) == geographic_info['SIZE']\n",
    "biggestByState = geographic_info[a]\n",
    "biggestByState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "fig = go.Figure(data=go.Choropleth(\n",
    "    locations=biggestByState['STATE'], # Spatial coordinates\n",
    "    z = biggestByState['merged_labels'].astype(float), # Data to be color-coded\n",
    "    text = biggestByState[[ \"name\", 'SIZE']],\n",
    "    locationmode = 'USA-states', # set of locations match entries in `locations`\n",
    "    colorscale = colors    \n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text = 'Cluster Dominance by State',\n",
    "    geo_scope='usa', # limite map scope to USA\n",
    "            showlegend = False\n",
    "\n",
    "    \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggestByState.loc[ biggestByState.STATE == 'AZ' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = clusters_merged[['STATE','ZIP','merged_labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_zip = pd.DataFrame(check.groupby(['ZIP','merged_labels']).size().to_frame('SIZE') )\n",
    "geo_zip = geo_zip.reset_index()\n",
    "geo_zip.ZIP = geo_zip.ZIP.apply(lambda x : x[ 0 : 5])\n",
    "geo_zip.ZIP = geo_zip.ZIP.astype(str).astype(int)\n",
    "fip  = pd.read_csv('data/zip_fip.csv')\n",
    "fip = fip[['ZIP','STCOUNTYFP','STATE']]\n",
    "fip = fip.rename(columns={\"STATE\": \"STATE_ZIP\", \"STCOUNTYFP\": \"FIP\"})\n",
    "geographic_fip = pd.merge(geo_zip, fip, left_on='ZIP', right_on='ZIP')\n",
    "geographic_fip\n",
    "\n",
    "a = geographic_fip.groupby(['ZIP'])['SIZE'].transform(max) == geographic_fip['SIZE']\n",
    "biggestByFIP = geographic_fip[a]\n",
    "biggestByFIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if False: \n",
    "\n",
    "    import plotly.figure_factory as ff\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "\n",
    "    fips = biggestByFIP['FIP'].tolist()\n",
    "    values = biggestByFIP['merged_labels'].tolist()\n",
    "\n",
    "\n",
    "    fig = ff.create_choropleth(\n",
    "        fips=fips, values=values, scope=['usa'],\n",
    "        colorscale=colors,\n",
    "        show_state_data=False,\n",
    "        show_hover=True,\n",
    "        asp = 2.9,\n",
    "        title_text = 'Clustering by ZIP Code',\n",
    "        legend_title = 'Clusters'\n",
    "    )\n",
    "\n",
    "    fig.layout.template = None\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: \n",
    "\n",
    "    import plotly.figure_factory as ff\n",
    "\n",
    "    fips = biggestByFIP.FIP.tolist()\n",
    "    values = biggestByFIP.merged_labels.tolist()\n",
    "\n",
    "    fig = ff.create_choropleth(fips=fips, values=values,\n",
    "        colorscale=colors ,\n",
    "        show_state_data=True,\n",
    "        show_hover=True, title_text = 'Clustering by ZIP CODE', legend_title = 'Clusters')\n",
    "    fig.layout.template = None\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
