{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "ipython = get_ipython()\n",
    "\n",
    "exec_no = ipython.execution_count\n",
    "exec_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random \n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from IPython.utils import io\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "\n",
    "#!pip install -U git+https://github.com/joaopfonseca/SOMPY.git\n",
    "import sompy\n",
    "from sompy.visualization.mapview import View2D\n",
    "from sompy.visualization.bmuhits import BmuHitsView\n",
    "from sompy.visualization.hitmap import HitMapView\n",
    "\n",
    "\n",
    "\n",
    "#%pip install boruta\n",
    "from boruta import BorutaPy\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # optionally, you can change 'svg' to 'retina'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: \n",
    "    with io.capture_output() as captured:\n",
    "        %run get_wd.py\n",
    "\n",
    "    s = captured.stdout # prints stdout from your script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exec_no == 1: \n",
    "    s = os.getcwd()\n",
    "    os.chdir(os.path.dirname(s))\n",
    "    exec_no += 1\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitions \n",
    "\n",
    "#os.chdir('/Users/dp/Nova/OneDrive - NOVAIMS/1stSemester/DM/DMProject')\n",
    "computed_data_path = 'computed_data/'\n",
    "explorations_data_path = 'explorations/'\n",
    "\n",
    "paths = [computed_data_path, explorations_data_path]\n",
    "for path in paths:\n",
    "    if not os.path.exists(path): \n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run promo_history.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(computed_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preparation results\n",
    "extended_data = False\n",
    "\n",
    "if extended_data: \n",
    "    with open(os.path.join(computed_data_path, 'history_feat_raw.pickle'), 'rb') as f: \n",
    "        history_agg = pickle.load(f)\n",
    "\n",
    "    with open(os.path.join(computed_data_path, 'history_feat_multi_out.pickle'), 'rb') as f: \n",
    "        history_agg_out_multi = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(computed_data_path, 'history_feat_multi_clean.pickle'), 'rb') as f: \n",
    "    history_agg_multi = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if extended_data: \n",
    "    with open(os.path.join(computed_data_path, 'neighborhood_feat_after_impute.pickle'), 'rb') as f: \n",
    "        neighborhood_feat_after_impute = pickle.load(f)\n",
    "\n",
    "    with open(os.path.join(computed_data_path, 'neighborhood_outliers.pickle'), 'rb') as f: \n",
    "        neighborhood_outliers = pickle.load(f)\n",
    "    neighborhood_outliers.index = neighborhood_outliers.index.astype(int)\n",
    "\n",
    "\n",
    "with open(os.path.join(computed_data_path, 'neighborhood_PC_cluster.pickle'), 'rb') as f: \n",
    "    neighborhood_PC_cluster = pickle.load(f)\n",
    "\n",
    "neighborhood_PC_cluster.index = neighborhood_PC_cluster.index.astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_k = False\n",
    "tnse_hist = False\n",
    "tnse_neigh = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tables(df_hist, df_neigh):\n",
    "    \n",
    "    hist_neigh_inter = df_neigh.index.intersection(df_hist.index)\n",
    "    len_hist_neigh_inter = len(hist_neigh_inter)\n",
    "    len_hist_neigh_inter_rel = len(hist_neigh_inter) / len(df_hist.index)\n",
    "    print('len_hist_neigh_inter: ', len_hist_neigh_inter)\n",
    "    print('len_hist_neigh_inter_rel: ', len_hist_neigh_inter_rel)\n",
    "\n",
    "\n",
    "    df_normal = df_hist.merge(df_neigh, how='inner', left_index=True, right_index=True)\n",
    "    return df_normal\n",
    "\n",
    "    print('df_normal.shape: ', df_normal.shape)\n",
    "    \n",
    "    \n",
    "def scale_df(df, scale='minmax'): \n",
    "    if scale == 'minmax': \n",
    "        scaler = MinMaxScaler()\n",
    "    df_normal = scaler.fit_transform(df)\n",
    "    df_normal = pd.DataFrame(df_normal, columns=df.columns, index=df.index)\n",
    "    return df_normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cl_feat_neigh = neighborhood_PC_cluster.columns.to_list()\n",
    "cl_feat_neigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set holding the class names\n",
    "cl_names = set()\n",
    "\n",
    "# define cols\n",
    "\n",
    "## cols for clsutering \n",
    "cl_feat_hist = history_agg_multi.columns.to_list()\n",
    "print('\\ncl_feat_hist: \\n',cl_feat_hist)\n",
    "cl_feat_neigh = neighborhood_PC_cluster.columns.to_list()\n",
    "\n",
    "metric_features = cl_feat_hist + cl_feat_neigh \n",
    "print('\\nmetric_features: ', metric_features)\n",
    "## cols for descirption \n",
    "#desc_feat_hist = history_agg.loc[~history_agg.columns.isin(cl_feat_hist),:].columns.to_list()\n",
    "if extended_data:\n",
    "    desc_feat_hist = history_agg.columns[~history_agg.columns.isin(cl_feat_hist)].to_list()\n",
    "    print('\\ndesc_feat_hist: \\n',desc_feat_hist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define rows\n",
    "\n",
    "cl_donor_ids = history_agg_multi.index.to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist = history_agg_multi[cl_feat_hist].copy()\n",
    "df_hist\n",
    "\n",
    "df_neigh = neighborhood_PC_cluster.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge / Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_scale = False # if false: first scale, than merge\n",
    "intersect_dfs = True # make sure both dfs have same ids\n",
    "\n",
    "\n",
    "if merge_scale:\n",
    "    df_raw = merge_tables(df_hist, df_neigh)\n",
    "    df_raw\n",
    "\n",
    "    df_normal = scale_df(df_raw)\n",
    "    df_normal\n",
    "\n",
    "    df_hist_normal = df_normal.copy()[cl_feat_hist]\n",
    "    df_hist_normal\n",
    "\n",
    "    df_neigh_normal = df_normal.copy()[cl_feat_neigh]\n",
    "    df_neigh_normal\n",
    "else: \n",
    "    \n",
    "    df_hist_normal = scale_df(df=df_hist, scale='minmax')\n",
    "    df_neigh_normal = scale_df(df=df_neigh, scale='minmax')\n",
    "    \n",
    "    df_normal = merge_tables(df_hist_normal, df_neigh_normal)\n",
    "    \n",
    "    if intersect_dfs:   \n",
    "\n",
    "        df_hist_normal = df_normal[cl_feat_hist].copy()\n",
    "        df_neigh_normal = df_normal[cl_feat_neigh].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the correct k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ss(df):\n",
    "    \"\"\"Computes the sum of squares for all variables given a dataset\n",
    "    \"\"\"\n",
    "    ss = np.sum(df.var() * (df.count() - 1))\n",
    "    return ss  # return sum of sum of squares of each df variable\n",
    "\n",
    "def r2(df, labels):\n",
    "    sst = get_ss(df)\n",
    "    ssw = np.sum(df.groupby(labels).apply(get_ss))\n",
    "    return 1 - ssw/sst\n",
    "    \n",
    "def get_r2_scores(df, clusterer, min_k=1, max_k=10):\n",
    "    \"\"\"\n",
    "    Loop over different values of k. To be used with sklearn clusterers.\n",
    "    \"\"\"\n",
    "    r2_clust = {}\n",
    "    for n in range(min_k, max_k):\n",
    "        clust = clone(clusterer).set_params(n_clusters=n)\n",
    "        labels = clust.fit_predict(df)\n",
    "        r2_clust[n] = r2(df, labels)\n",
    "    return r2_clust\n",
    "\n",
    "\n",
    "# Set up the clusterers\n",
    "kmeans = KMeans(\n",
    "    init='k-means++',\n",
    "    n_init=20,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "hierarchical = AgglomerativeClustering(\n",
    "    affinity='euclidean'\n",
    ")\n",
    "\n",
    "def find_k_r2(df):\n",
    "    r2_scores = {}\n",
    "    \n",
    "    print('kmeans')\n",
    "    r2_scores['kmeans'] = get_r2_scores(df, kmeans)\n",
    "\n",
    "    if False: \n",
    "        for linkage in ['ward']: # 'complete', 'average', 'single',\n",
    "            print(linkage)\n",
    "            r2_scores[linkage] = get_r2_scores(\n",
    "                df, hierarchical.set_params(linkage=linkage)\n",
    "            )\n",
    "\n",
    "    pd.DataFrame(r2_scores)\n",
    "    return r2_scores\n",
    "\n",
    "# Visualizing the R² scores for each cluster solution on demographic variables\n",
    "def plot_k_r2(r2_scores):\n",
    "    filename = r2_scores\n",
    "    r2_scores = globals()[r2_scores]\n",
    "    pd.DataFrame(r2_scores).plot.line(figsize=(10,7))\n",
    "\n",
    "    #plt.title(\"Demographic Variables:\\nR² plot for various clustering methods\\n\", fontsize=21)\n",
    "    #plt.legend(title=\"Cluster methods\", title_fontsize=11)\n",
    "    plt.xlabel(\"Number of clusters\", fontsize=13)\n",
    "    plt.ylabel(\"R² metric\", fontsize=13)\n",
    "    plt.savefig(os.path.join(explorations_data_path, f'{filename}.jpeg'), dpi=200)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def find_k_inertia(df): \n",
    "    range_clusters = range(1, 11)\n",
    "    inertia = []\n",
    "    for n_clus in range_clusters:  # iterate over desired ncluster range\n",
    "        print(n_clus)\n",
    "        kmclust = KMeans(n_clusters=n_clus, init='k-means++', n_init=20, random_state=1)\n",
    "        kmclust.fit(df)\n",
    "        inertia.append(kmclust.inertia_)  # save the inertia of the given cluster solution\n",
    "    return inertia\n",
    "\n",
    "def plot_k_inertia(inertia): \n",
    "    filename = inertia\n",
    "    inertia = globals()[inertia]\n",
    "    plt.plot(inertia)\n",
    "    plt.xlabel(\"Number of clusters\", fontsize=13)\n",
    "    plt.ylabel(\"Inertia\", fontsize=13)\n",
    "    plt.savefig(os.path.join(explorations_data_path, f'{filename}.jpeg'), dpi=200)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the optimal clusterer on promotion history variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist\n",
    "if find_k:\n",
    "    #r2\n",
    "    if True: \n",
    "        r2_scores_hist = find_k_r2(df=df_hist_normal)\n",
    "        plot_k_r2('r2_scores_hist')\n",
    "\n",
    "    if True:\n",
    "        # inertia\n",
    "        inertia_hist = find_k_inertia(df=df_hist_normal)    \n",
    "        plot_k_inertia(inertia='inertia_hist')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neigh\n",
    "if find_k:\n",
    "    #r2\n",
    "    if True: \n",
    "        r2_scores_neigh = find_k_r2(df=df_neigh_normal)\n",
    "        plot_k_r2('r2_scores_neigh')\n",
    "\n",
    "    # inertia\n",
    "    if True:\n",
    "        inertia_neigh = find_k_inertia(df=df_neigh_normal)    \n",
    "        plot_k_inertia(inertia='inertia_neigh')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som = False\n",
    "\n",
    "if som:\n",
    "\n",
    "    # This som implementation does not have a random seed parameter\n",
    "    # We're going to set it up ourselves\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Notice that the SOM did not converge - We're under a time constraint for this class\n",
    "    sm = sompy.SOMFactory().build(\n",
    "        df[metric_features].values, \n",
    "        mapsize=(50, 50), \n",
    "        initialization='random',\n",
    "        neighborhood='gaussian',\n",
    "        training='batch',\n",
    "        lattice='hexa',\n",
    "        component_names=metric_features\n",
    "    )\n",
    "    sm.train(n_job=-1, verbose='info', train_rough_len=100, train_finetune_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if som:\n",
    "\n",
    "    # Coordinates of the units in the input space\n",
    "    sm.get_node_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if som:\n",
    "\n",
    "    # Component planes on the 50x50 grid\n",
    "    sns.set()\n",
    "    view2D = View2D(12,12,\"\", text_size=10)\n",
    "    view2D.show(sm, col_sz=3, what='codebook')\n",
    "    plt.subplots_adjust(top=0.90)\n",
    "    plt.suptitle(\"Component Planes\", fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if som:\n",
    "\n",
    "    # U-matrix of the 50x50 grid\n",
    "    u = sompy.umatrix.UMatrixView(12, 12, 'umatrix', show_axis=True, text_size=8, show_text=True)\n",
    "\n",
    "    UMAT = u.show(\n",
    "        sm, \n",
    "        distance2=1, \n",
    "        row_normalized=False, \n",
    "        show_data=False, \n",
    "        contooor=True # Visualize isomorphic curves\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the right clustering (algorithm and number of clusters) for each perspective\n",
    "kmeans_hist = KMeans(\n",
    "    n_clusters=2,\n",
    "    init='k-means++',\n",
    "    n_init=20,\n",
    "    random_state=1\n",
    ")\n",
    "hist_labels = kmeans_hist.fit_predict(df_hist_normal)\n",
    "\n",
    "cl_name = 'hist_labels'\n",
    "if intersect_dfs:\n",
    "    df_normal[cl_name] = hist_labels\n",
    "df_hist_normal[cl_name] = hist_labels\n",
    "\n",
    "cl_names.add(cl_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the right clustering (algorithm and number of clusters) for each perspective\n",
    "kmeans_neigh = KMeans(\n",
    "    n_clusters=2,\n",
    "    init='k-means++',\n",
    "    n_init=20,\n",
    "    random_state=1\n",
    ")\n",
    "neigh_labels = kmeans_neigh.fit_predict(df_neigh_normal)\n",
    "\n",
    "cl_name = 'neigh_labels'\n",
    "if intersect_dfs:\n",
    "    df_normal[cl_name] = neigh_labels\n",
    "df_neigh_normal[cl_name] = neigh_labels\n",
    "\n",
    "cl_names.add(cl_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal[cl_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster visualization using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tsne_data(df, features, label_name, fraq): \n",
    "\n",
    "    tsne_df = df.sample(frac=fraq, axis=0, random_state=1)\n",
    "    tsne_feat = tsne_df[features]\n",
    "    tsne_feat\n",
    "\n",
    "    tsne_c = tsne_df[label_name]\n",
    "    tsne_c\n",
    "    \n",
    "    return tsne_feat, tsne_c\n",
    "\n",
    "def run_tsne(tsne_feat, tsne_c, filename): \n",
    "    two_dim = TSNE(random_state=1, n_jobs=-1).fit_transform(tsne_feat)\n",
    "    # t-SNE visualization\n",
    "    pd.DataFrame(two_dim).plot.scatter(x=0, y=1, c=tsne_c, colormap='tab10', figsize=(15,10))\n",
    "    plt.savefig(os.path.join(explorations_data_path, f'tsne_{filename}.jpeg'), dpi=200)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neigh_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if tnse_hist:\n",
    "    tsne_feat, tsne_c = prepare_tsne_data(df=df_hist_normal, features=cl_feat_hist, label_name='hist_labels', fraq=.1)\n",
    "    run_tsne(tsne_feat, tsne_c, filename='hist')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if tnse_neigh:\n",
    "    tsne_feat, tsne_c = prepare_tsne_data(df=df_neigh_normal, features=cl_feat_neigh, label_name='neigh_labels', fraq=.1)\n",
    "    run_tsne(tsne_feat, tsne_c, filename='neigh')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging using Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_names_ll = list(cl_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centroids of the concatenated cluster labels\n",
    "df_centroids = df_normal.groupby(cl_names_ll)\\\n",
    "    [metric_features].mean()\n",
    "df_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Hierarchical clustering to merge the concatenated cluster centroids\n",
    "hclust = AgglomerativeClustering(\n",
    "    linkage='ward', \n",
    "    affinity='euclidean', \n",
    "    distance_threshold=0, \n",
    "    n_clusters=None\n",
    ")\n",
    "hclust_labels = hclust.fit_predict(df_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py\n",
    "\n",
    "# create the counts of samples under each node (number of points being merged)\n",
    "counts = np.zeros(hclust.children_.shape[0])\n",
    "n_samples = len(hclust.labels_)\n",
    "\n",
    "# hclust.children_ contains the observation ids that are being merged together\n",
    "# At the i-th iteration, children[i][0] and children[i][1] are merged to form node n_samples + i\n",
    "for i, merge in enumerate(hclust.children_):\n",
    "    # track the number of observations in the current cluster being formed\n",
    "    current_count = 0\n",
    "    for child_idx in merge:\n",
    "        if child_idx < n_samples:\n",
    "            # If this is True, then we are merging an observation\n",
    "            current_count += 1  # leaf node\n",
    "        else:\n",
    "            # Otherwise, we are merging a previously formed cluster\n",
    "            current_count += counts[child_idx - n_samples]\n",
    "    counts[i] = current_count\n",
    "\n",
    "# the hclust.children_ is used to indicate the two points/clusters being merged (dendrogram's u-joins)\n",
    "# the hclust.distances_ indicates the distance between the two points/clusters (height of the u-joins)\n",
    "# the counts indicate the number of points being merged (dendrogram's x-axis)\n",
    "linkage_matrix = np.column_stack(\n",
    "    [hclust.children_, hclust.distances_, counts]\n",
    ").astype(float)\n",
    "\n",
    "# Plot the corresponding dendrogram\n",
    "sns.set()\n",
    "fig = plt.figure()# figsize=(11,5)\n",
    "# The Dendrogram parameters need to be tuned\n",
    "y_threshold = .4\n",
    "dendrogram(linkage_matrix, truncate_mode='level', labels=df_centroids.index.to_list(), p=5, color_threshold=y_threshold, above_threshold_color='k')\n",
    "plt.hlines(y_threshold, 0, 1000, colors=\"r\", linestyles=\"dashed\")\n",
    "#plt.title(f'Hierarchical Clustering - {linkage.title()}\\'s Dendrogram', fontsize=21)\n",
    "plt.xlabel('Number of points in node (or index of point if no parenthesis)')\n",
    "plt.ylabel(f'Euclidean Distance', fontsize=13)\n",
    "plt.savefig(os.path.join(explorations_data_path, 'dendogram_comb.jpeg'), dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrogram(linkage_matrix, truncate_mode='level', labels=df_centroids.index.to_list(), p=5, color_threshold=y_threshold, above_threshold_color='k')\n",
    "linkage_matrix.shape\n",
    "df_centroids.index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-running the Hierarchical clustering based on the correct number of clusters\n",
    "hclust = AgglomerativeClustering(\n",
    "    linkage='ward', \n",
    "    affinity='euclidean', \n",
    "    n_clusters=4\n",
    ")\n",
    "hclust_labels = hclust.fit_predict(df_centroids)\n",
    "df_centroids['hclust_labels'] = hclust_labels\n",
    "\n",
    "df_centroids  # centroid's cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapper between concatenated clusters and hierarchical clusters\n",
    "cluster_mapper = df_centroids['hclust_labels'].to_dict()\n",
    "\n",
    "df_ = df_normal.copy()\n",
    "\n",
    "# Mapping the hierarchical clusters on the centroids to the observations\n",
    "df_['merged_labels'] = df_.apply(\n",
    "    lambda row: cluster_mapper[\n",
    "        (row['neigh_labels'], row['hist_labels'])\n",
    "    ], axis=1\n",
    ")\n",
    "\n",
    "# Merged cluster centroids\n",
    "df_.groupby('merged_labels').mean()[metric_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_mapper_df = df_centroids['hclust_labels'].to_frame()\n",
    "cluster_mapper_df.to_excel(os.path.join(explorations_data_path, 'cluster_mapper.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge cluster contigency table\n",
    "# Getting size of each final cluster\n",
    "df_counts = df_.groupby('merged_labels')\\\n",
    "    .size()\\\n",
    "    .to_frame()\n",
    "\n",
    "# Getting the product and behavior labels\n",
    "df_counts = df_counts\\\n",
    "    .rename({v:k for k, v in cluster_mapper.items()})\\\n",
    "    .reset_index()\n",
    "\n",
    "df_counts['neigh_labels'] = df_counts['merged_labels'].apply(lambda x: x[0])\n",
    "df_counts['hist_labels'] = df_counts['merged_labels'].apply(lambda x: x[1])\n",
    "df_counts.pivot('neigh_labels', 'hist_labels', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting df to have the final product, behavior and merged clusters\n",
    "df_normal = df_.copy()\n",
    "df_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_names.add('merged_labels')\n",
    "cl_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_names_ll = list(cl_names)\n",
    "cl_names_ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_[cl_names].to_csv(os.path.join(explorations_data_path, 'cluster_donor_mapping.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feat importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/scikit-learn-contrib/boruta_py\n",
    "# https://towardsdatascience.com/feature-selection-with-borutapy-f0ea84c9366\n",
    "\n",
    "\n",
    "# load X and y\n",
    "# NOTE BorutaPy accepts numpy arrays only, hence the .values attribute\n",
    "# X = pd.read_csv('examples/test_X.csv', index_col=0).values\n",
    "# y = pd.read_csv('examples/test_y.csv', header=None, index_col=0).values\n",
    "# y = y.ravel()\n",
    "\n",
    "def feat_imp_boruta(X_df,y_s, boruta:True): \n",
    "    if boruta: \n",
    "        X_features = X_df.columns.to_list()\n",
    "        X = X_df.values\n",
    "        y = y_s.values\n",
    "\n",
    "        print('X.shape:', X.shape)\n",
    "\n",
    "        # define random forest classifier, with utilising all cores and\n",
    "        # sampling in proportion to y labels\n",
    "        rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n",
    "\n",
    "        # define Boruta feature selection method\n",
    "        feat_selector = BorutaPy(rf, n_estimators='auto', verbose=0, random_state=1)\n",
    "\n",
    "        # find all relevant features - 5 features should be selected\n",
    "        feat_selector.fit(X, y)\n",
    "\n",
    "        # check selected features - first 5 features are selected\n",
    "        print(feat_selector.support_)\n",
    "        # check ranking of features\n",
    "        print(feat_selector.ranking_)\n",
    "\n",
    "        # call transform() on X to filter it down to selected features\n",
    "        X = feat_selector.transform(X)\n",
    "\n",
    "\n",
    "        # impt\n",
    "        X_metadata = pd.DataFrame({\n",
    "        'features': (X_features),\n",
    "        'support_b': (feat_selector.support_) \n",
    "        #,'ranking': (feat_selector.ranking_)\n",
    "        })\n",
    "\n",
    "\n",
    "        print(X_metadata.support_b.sum())\n",
    "\n",
    "        imp_b = X_metadata.set_index('features')\n",
    "\n",
    "    else: \n",
    "        imp_b = pd.DataFrame({'support_b':[np.nan]}, index=X_df.columns)\n",
    "        imp_b.index.set_names('features', inplace=True)\n",
    "        imp_b\n",
    "    \n",
    "    return imp_b\n",
    "\n",
    "def feat_imp_std(df, cl_name, th):\n",
    "    filename = cl_name.replace('_labels', '')\n",
    "    \n",
    "    imp = df.groupby(cl_name).mean().std().sort_values(ascending=False)\n",
    "    imp = imp.to_frame(name='std')\n",
    "    imp['std_cumsum'] = imp.cumsum() / imp.cumsum().max()\n",
    "    imp\n",
    "\n",
    "    sns.lineplot(data=imp, y=imp.index, x='std_cumsum')\n",
    "    #plt.xticks(rotation=90)\n",
    "    plt.axvline(th)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(explorations_data_path, f'feat_imp_{filename}.jpeg'), dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "    imp['support_std'] = [True if v <=th else False for v in imp.std_cumsum]\n",
    "    imp\n",
    "    imp.index.set_names('features', inplace=True)\n",
    "    return imp \n",
    "\n",
    "\n",
    "def feat_imp(df, cl_name, cl_names_all, boruta, th): \n",
    "    cl_drop = [x for x in cl_names_all if x != cl_name]\n",
    "    #print('cl_drop: ', cl_drop)\n",
    "    df_ = df.copy().drop(columns=cl_drop, errors='ignore') # \n",
    "\n",
    "    #print(df_.head())\n",
    "    \n",
    "    # boruta\n",
    "    imp_b = feat_imp_boruta(\n",
    "        X_df=df_.drop(columns=cl_name, errors='ignore'),\n",
    "        y_s=df_[cl_name], \n",
    "        boruta = boruta\n",
    "    )\n",
    "\n",
    "\n",
    "    # std\n",
    "    imp_std = feat_imp_std(\n",
    "        df = df_,\n",
    "        cl_name = cl_name, \n",
    "        th = th\n",
    "    )\n",
    "\n",
    "\n",
    "    # combine\n",
    "    imp_comb = pd.concat([imp_b, imp_std], 1)\n",
    "    \n",
    "    # overall support\n",
    "    cols_with_support = ['support_b', 'support_std']\n",
    "    imp_comb['support'] = imp_comb[cols_with_support].apply(lambda x: np.sum(x), axis=1)\n",
    "    \n",
    "    return imp_comb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history\n",
    "\n",
    "imp_comb_hist = feat_imp(df=df_hist_normal,cl_name='hist_labels',cl_names_all=cl_names, boruta=False, th=.8)\n",
    "imp_comb_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neigh\n",
    "\n",
    "imp_comb_neigh = feat_imp(df=df_neigh_normal,cl_name='neigh_labels',cl_names_all=cl_names, boruta=False, th=0.93)\n",
    "imp_comb_neigh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged \n",
    "imp_comb_merged = feat_imp(df=df_normal,cl_name='merged_labels',cl_names_all=cl_names, boruta=False, th=0.85)\n",
    "imp_comb_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## leverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_leverage_values(x): \n",
    "    d = {}\n",
    "    d['total_donations'] = x['donations_total'].sum()\n",
    "    d['size'] = x.size\n",
    "    return pd.Series(d)\n",
    "\n",
    "def build_leverage_df(df, cl_name): \n",
    "    leverage_df = df.groupby(cl_name).apply(calc_leverage_values)\n",
    "    leverage_df['total_donations_rel'] = leverage_df['total_donations'] / leverage_df['total_donations'].sum()\n",
    "    leverage_df['size_rel'] = leverage_df['size'] / leverage_df['size'].sum()\n",
    "\n",
    "    leverage_df['leverage'] = round(leverage_df['total_donations_rel'] / leverage_df['size_rel'], 2)\n",
    "    leverage_df.reset_index(inplace=True)\n",
    "    return leverage_df\n",
    "\n",
    "def build_leverage_plotdata(leverage_df, cl_name): \n",
    "\n",
    "    # bars\n",
    "    lev_plotdata = leverage_df[[cl_name, 'total_donations_rel', 'size_rel']].melt(id_vars=cl_name)\n",
    "    lev_plotdata[cl_name] = lev_plotdata[cl_name].astype(str)\n",
    "\n",
    "    # text annotations\n",
    "    max_val_per_cl = leverage_df[['total_donations_rel', 'size_rel']].max(1)\n",
    "    dist = max_val_per_cl.max() * 0.05\n",
    "    y_coord = max_val_per_cl + dist\n",
    "    y_coord_df = y_coord.reset_index(name='y_coord')\n",
    "    y_coord_df.rename(columns={'index':cl_name}, inplace=True)\n",
    "\n",
    "    lev_text = leverage_df[[cl_name, 'leverage']]\n",
    "    #lev_text.loc[:,'y_coord'] = y_coord\n",
    "    lev_text = lev_text.merge(y_coord_df, on=cl_name)\n",
    "    lev_text\n",
    "\n",
    "    return lev_plotdata, lev_text\n",
    "\n",
    "def plot_leverage(lev_plotdata, lev_text, cl_name, plot_name): \n",
    "\n",
    "    g = sns.barplot(data=lev_plotdata, x = cl_name, y='value', hue='variable')\n",
    "\n",
    "    for _, (x,s,y) in lev_text.iterrows():\n",
    "        plt.text(x=x,y=y, s=s, horizontalalignment='center')\n",
    "\n",
    "    g.set_ylim(top=g.get_ylim()[1]*1.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #print(g.get_ylim())\n",
    "    plt.savefig(os.path.join(explorations_data_path, f'leverage_{plot_name}.jpeg'), dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "def build_plot_leverage(df, cl_name, plot_name): \n",
    "    leverage_df = build_leverage_df(df, cl_name)\n",
    "    lev_plotdata, lev_text = build_leverage_plotdata(leverage_df, cl_name)\n",
    "    plot_leverage(lev_plotdata, lev_text, cl_name, plot_name)\n",
    "    return leverage_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "leverage_df = build_plot_leverage(\n",
    "    df=df_normal, \n",
    "    cl_name='hist_labels', \n",
    "    plot_name='hist_test'\n",
    ")\n",
    "\n",
    "#leverage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "leverage_df = build_plot_leverage(\n",
    "    df=df_normal, \n",
    "    cl_name='neigh_labels', \n",
    "    plot_name='neigh'\n",
    ")\n",
    "\n",
    "leverage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "leverage_df = build_plot_leverage(\n",
    "    df=df_normal, \n",
    "    cl_name='merged_labels', \n",
    "    plot_name='merged'\n",
    ")\n",
    "\n",
    "leverage_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_profiles(df, label_columns, figsize, compar_titles=None):\n",
    "    \"\"\"\n",
    "    Pass df with labels columns of one or multiple clustering labels. \n",
    "    Then specify this label columns to perform the cluster profile according to them.\n",
    "    \"\"\"\n",
    "    if compar_titles == None:\n",
    "        compar_titles = [\"\"]*len(label_columns)\n",
    "        \n",
    "    sns.set()\n",
    "    fig, axes = plt.subplots(nrows=len(label_columns), ncols=2, figsize=figsize, squeeze=False)\n",
    "    for ax, label, titl in zip(axes, label_columns, compar_titles):\n",
    "        # Filtering df\n",
    "        drop_cols = [i for i in label_columns if i!=label]\n",
    "        dfax = df.drop(drop_cols, axis=1)\n",
    "        \n",
    "        # Getting the cluster centroids and counts\n",
    "        centroids = dfax.groupby(by=label, as_index=False).mean()\n",
    "        counts = dfax.groupby(by=label, as_index=False).count().iloc[:,[0,1]]\n",
    "        counts.columns = [label, \"counts\"]\n",
    "        \n",
    "        # Setting Data\n",
    "        pd.plotting.parallel_coordinates(centroids, label, color=sns.color_palette(), ax=ax[0])\n",
    "        sns.barplot(x=label, y=\"counts\", data=counts, ax=ax[1])\n",
    "\n",
    "        #Setting Layout\n",
    "        handles, _ = ax[0].get_legend_handles_labels()\n",
    "        cluster_labels = [\"Cluster {}\".format(i) for i in range(len(handles))]\n",
    "        #print('cluster_labels: ', cluster_labels)\n",
    "        \n",
    "        color_mapper = {}\n",
    "        for handle in handles: \n",
    "            color_mapper[handle.get_label()] = handle.get_color()\n",
    "        \n",
    "        #print('handles: ', handles[0].get_color())\n",
    "        #print('handles: ', handles[0].get_label())\n",
    "\n",
    "\n",
    "        ax[0].annotate(text=titl, xy=(0.95,1.1), xycoords='axes fraction', fontsize=13, fontweight = 'heavy') \n",
    "        ax[0].legend(handles, cluster_labels) # Adaptable to number of clusters\n",
    "        ax[0].axhline(color=\"black\", linestyle=\"--\")\n",
    "        ax[0].set_title(\"Cluster Means - {} Clusters\".format(len(handles)), fontsize=13)\n",
    "        ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=30)\n",
    "        ax[1].set_xticklabels(cluster_labels)\n",
    "        ax[1].set_xlabel(\"\")\n",
    "        ax[1].set_ylabel(\"Absolute Frequency\")\n",
    "        ax[1].set_title(\"Cluster Sizes - {} Clusters\".format(len(handles)), fontsize=13)\n",
    "    \n",
    "    #plt.subplots_adjust(hspace=0.4, top=0.90)\n",
    "    plt.tight_layout()\n",
    "    plt.title = ''\n",
    "#    plt.suptitle(\"Cluster Simple Profilling\", fontsize=23)\n",
    "\n",
    "    filename = ''.join(label_columns)\n",
    "    plt.savefig(os.path.join(explorations_data_path, f'profile_mean_{filename}.jpeg'), dpi=200)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return color_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_comb_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history\n",
    "feat_supp_hist = imp_comb_hist.loc[imp_comb_hist.support >= 1,].index.to_list()\n",
    "print('feat_supp_hist: ', feat_supp_hist)\n",
    "cl_name_hist = 'hist_labels'\n",
    "#df_hist_normal[feat_supp_hist + [cl_name_hist]].groupby(cl_name_hist).mean().T\n",
    "\n",
    "my_dpi = 200\n",
    "figsize=(\n",
    "    # 15,13\n",
    "    1800/my_dpi, 1000/my_dpi\n",
    ")\n",
    "    \n",
    "cp = cluster_profiles(\n",
    "    df = df_hist_normal[feat_supp_hist + [cl_name_hist]], \n",
    "    label_columns = [cl_name_hist], \n",
    "    figsize = figsize, #None,#(28, 13), \n",
    "    compar_titles = [\"History clustering\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neigh\n",
    "feat_supp_neigh = imp_comb_neigh.loc[imp_comb_neigh.support >= 1,].index.to_list()\n",
    "print('feat_supp_neigh: ', feat_supp_neigh)\n",
    "cl_name_neigh = 'neigh_labels'\n",
    "#df_hist_normal[feat_supp_hist + [cl_name_hist]].groupby(cl_name_hist).mean().T\n",
    "\n",
    "my_dpi = 200\n",
    "figsize=(\n",
    "    # 15,13\n",
    "    1800/my_dpi, 1000/my_dpi\n",
    ")\n",
    "    \n",
    "cp = cluster_profiles(\n",
    "    df = df_neigh_normal[feat_supp_neigh + [cl_name_neigh]], \n",
    "    label_columns = [cl_name_neigh], \n",
    "    figsize = figsize, #None,#(28, 13), \n",
    "    compar_titles = [\"Neighborhood clustering\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged\n",
    "feat_supp_merged = imp_comb_merged.loc[imp_comb_merged.support >= 1,].index.to_list()\n",
    "print('feat_supp_neigh: ', feat_supp_merged)\n",
    "cl_name_merged = 'merged_labels'\n",
    "#df_hist_normal[feat_supp_hist + [cl_name_hist]].groupby(cl_name_hist).mean().T\n",
    "\n",
    "my_dpi = 200\n",
    "figsize=(\n",
    "    # 15,13\n",
    "    1800/my_dpi, 1000/my_dpi\n",
    ")\n",
    "    \n",
    "cp = cluster_profiles(\n",
    "    df = df_normal[feat_supp_merged + [cl_name_merged]], \n",
    "    label_columns = [cl_name_merged], \n",
    "    figsize = figsize, #None,#(28, 13), \n",
    "    compar_titles = [\"Merged clustering\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdata = pd.DataFrame(cp).T.sort_values(0)\n",
    "plotdata\n",
    "sns.countplot(x = plotdata.index, palette=cp)\n",
    "#list(cp.keys())\n",
    "\n",
    "with open(os.path.join(explorations_data_path, 'cluster_color_mapper.pickle'), 'wb') as f: \n",
    "    pickle.dump(cp, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import rgb2hex\n",
    "rgb2hex((0.8666666666666667, 0.5176470588235295, 0.3215686274509804))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_mapper_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_normal[feat_supp_merged + [cl_name_merged]].groupby(cl_name_merged).apply(lambda x: round(np.mean(x), 4))\\\n",
    "    .to_excel(os.path.join(explorations_data_path, 'feat_mean_per_cluster.xlsx'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
