{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "ipython = get_ipython()\n",
    "\n",
    "exec_no = ipython.execution_count\n",
    "exec_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random \n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from IPython.utils import io\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#!pip install -U git+https://github.com/joaopfonseca/SOMPY.git\n",
    "import sompy\n",
    "from sompy.visualization.mapview import View2D\n",
    "from sompy.visualization.bmuhits import BmuHitsView\n",
    "from sompy.visualization.hitmap import HitMapView\n",
    "\n",
    "%pip install boruta\n",
    "from boruta import BorutaPy\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # optionally, you can change 'svg' to 'retina'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: \n",
    "    with io.capture_output() as captured:\n",
    "        %run get_wd.py\n",
    "\n",
    "    s = captured.stdout # prints stdout from your script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exec_no == 1: \n",
    "    s = os.getcwd()\n",
    "    os.chdir(os.path.dirname(s))\n",
    "    exec_no += 1\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitions \n",
    "\n",
    "#os.chdir('/Users/dp/Nova/OneDrive - NOVAIMS/1stSemester/DM/DMProject')\n",
    "computed_data_path = 'computed_data/'\n",
    "explorations_data_path = 'explorations/'\n",
    "\n",
    "paths = [computed_data_path, explorations_data_path]\n",
    "for path in paths:\n",
    "    if not os.path.exists(path): \n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run promo_history.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "\n",
    "with open(os.path.join(computed_data_path, 'history_feat_raw.pickle'), 'rb') as f: \n",
    "    history_agg = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(computed_data_path, 'history_feat_multi_out.pickle'), 'rb') as f: \n",
    "    history_agg_out_multi = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(computed_data_path, 'history_feat_multi_clean.pickle'), 'rb') as f: \n",
    "    history_agg_multi = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_agg_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_agg_out_multi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cols\n",
    "\n",
    "## cols for clsutering \n",
    "cl_feat_hist = history_agg_multi.columns.to_list()\n",
    "print('\\ncl_feat_hist: \\n',cl_feat_hist)\n",
    "\n",
    "metric_features = cl_feat_hist\n",
    "\n",
    "## cols for descirption \n",
    "#desc_feat_hist = history_agg.loc[~history_agg.columns.isin(cl_feat_hist),:].columns.to_list()\n",
    "desc_feat_hist = history_agg.columns[~history_agg.columns.isin(cl_feat_hist)].to_list()\n",
    "print('\\ndesc_feat_hist: \\n',desc_feat_hist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define rows\n",
    "\n",
    "cl_donor_ids = history_agg_multi.index.to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_agg[history_agg.index.isin(cl_donor_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist = history_agg_multi[cl_feat_hist]\n",
    "df_hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_df(df, scale='minmax'): \n",
    "    if scale == 'minmax': \n",
    "        scaler = MinMaxScaler()\n",
    "    df_normal = scaler.fit_transform(df)\n",
    "    df_normal = pd.DataFrame(df_normal, columns=df.columns, index=df.index)\n",
    "    return df_normal\n",
    "\n",
    "    \n",
    "df_hist_normal = scale_df(df=df_hist, scale='minmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal = df_hist_normal.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multivariate outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_names = set()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the right clustering (algorithm and number of clusters) for each perspective\n",
    "kmeans_prod = KMeans(\n",
    "    n_clusters=3,\n",
    "    init='k-means++',\n",
    "    n_init=20,\n",
    "    random_state=1\n",
    ")\n",
    "hist_labels = kmeans_prod.fit_predict(df_hist_normal)\n",
    "\n",
    "cl_name = 'hist_labels'\n",
    "df_normal[cl_name] = hist_labels\n",
    "df_hist_normal[cl_name] = hist_labels\n",
    "\n",
    "cl_names.add(cl_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist_normal.hist_labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on K-means and Hierarchical clustering\n",
    "Based on (1) our previous tests and (2) the context of this problem, the optimal number of clusters is expected to be between 3 and 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ss(df):\n",
    "    \"\"\"Computes the sum of squares for all variables given a dataset\n",
    "    \"\"\"\n",
    "    ss = np.sum(df.var() * (df.count() - 1))\n",
    "    return ss  # return sum of sum of squares of each df variable\n",
    "\n",
    "def r2(df, labels):\n",
    "    sst = get_ss(df)\n",
    "    ssw = np.sum(df.groupby(labels).apply(get_ss))\n",
    "    return 1 - ssw/sst\n",
    "    \n",
    "def get_r2_scores(df, clusterer, min_k=2, max_k=10):\n",
    "    \"\"\"\n",
    "    Loop over different values of k. To be used with sklearn clusterers.\n",
    "    \"\"\"\n",
    "    r2_clust = {}\n",
    "    for n in range(min_k, max_k):\n",
    "        clust = clone(clusterer).set_params(n_clusters=n)\n",
    "        labels = clust.fit_predict(df)\n",
    "        r2_clust[n] = r2(df, labels)\n",
    "    return r2_clust\n",
    "\n",
    "\n",
    "# Set up the clusterers\n",
    "kmeans = KMeans(\n",
    "    init='k-means++',\n",
    "    n_init=20,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "hierarchical = AgglomerativeClustering(\n",
    "    affinity='euclidean'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the optimal clusterer on promotion history variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the R² scores for each cluster solution on promotion history variables\n",
    "find_hist_k = False\n",
    "if find_hist_k:\n",
    "    r2_scores = {}\n",
    "    \n",
    "    print('kmeans')\n",
    "    r2_scores['kmeans'] = get_r2_scores(df_hist_normal, kmeans)\n",
    "\n",
    "    if False: \n",
    "        for linkage in ['ward']: # 'complete', 'average', 'single',\n",
    "            print(linkage)\n",
    "            r2_scores[linkage] = get_r2_scores(\n",
    "                df_hist_normal, hierarchical.set_params(linkage=linkage)\n",
    "            )\n",
    "\n",
    "    pd.DataFrame(r2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the R² scores for each cluster solution on demographic variables\n",
    "if find_hist_k:\n",
    "    pd.DataFrame(r2_scores).plot.line(figsize=(10,7))\n",
    "\n",
    "    plt.title(\"Demographic Variables:\\nR² plot for various clustering methods\\n\", fontsize=21)\n",
    "    plt.legend(title=\"Cluster methods\", title_fontsize=11)\n",
    "    plt.xlabel(\"Number of clusters\", fontsize=13)\n",
    "    plt.ylabel(\"R² metric\", fontsize=13)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = False\n",
    "\n",
    "if inertia:\n",
    "    range_clusters = range(1, 11)\n",
    "    inertia = []\n",
    "    for n_clus in range_clusters:  # iterate over desired ncluster range\n",
    "        print(n_clus)\n",
    "        kmclust = KMeans(n_clusters=n_clus, init='k-means++', n_init=20, random_state=1)\n",
    "        kmclust.fit(df_normal[cl_feat_hist])\n",
    "        inertia.append(kmclust.inertia_)  # save the inertia of the given cluster solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if inertia:\n",
    "    plt.plot(inertia)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som = False\n",
    "\n",
    "if som:\n",
    "\n",
    "    # This som implementation does not have a random seed parameter\n",
    "    # We're going to set it up ourselves\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Notice that the SOM did not converge - We're under a time constraint for this class\n",
    "    sm = sompy.SOMFactory().build(\n",
    "        df[metric_features].values, \n",
    "        mapsize=(50, 50), \n",
    "        initialization='random',\n",
    "        neighborhood='gaussian',\n",
    "        training='batch',\n",
    "        lattice='hexa',\n",
    "        component_names=metric_features\n",
    "    )\n",
    "    sm.train(n_job=-1, verbose='info', train_rough_len=100, train_finetune_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if som:\n",
    "\n",
    "    # Coordinates of the units in the input space\n",
    "    sm.get_node_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if som:\n",
    "\n",
    "    # Component planes on the 50x50 grid\n",
    "    sns.set()\n",
    "    view2D = View2D(12,12,\"\", text_size=10)\n",
    "    view2D.show(sm, col_sz=3, what='codebook')\n",
    "    plt.subplots_adjust(top=0.90)\n",
    "    plt.suptitle(\"Component Planes\", fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if som:\n",
    "\n",
    "    # U-matrix of the 50x50 grid\n",
    "    u = sompy.umatrix.UMatrixView(12, 12, 'umatrix', show_axis=True, text_size=8, show_text=True)\n",
    "\n",
    "    UMAT = u.show(\n",
    "        sm, \n",
    "        distance2=1, \n",
    "        row_normalized=False, \n",
    "        show_data=False, \n",
    "        contooor=True # Visualize isomorphic curves\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster visualization using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnse = False\n",
    "\n",
    "if tnse:\n",
    "    label_name = 'hist_labels' # 'merged_labels'\n",
    "\n",
    "    tsne_df = df_normal.sample(frac=.1, axis=0, random_state=1)\n",
    "    tsne_feat = tsne_df[metric_features]\n",
    "    tsne_feat\n",
    "\n",
    "    tsne_c = tsne_df[label_name]\n",
    "    tsne_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tnse:\n",
    "\n",
    "    # This is step can be quite time consuming\n",
    "\n",
    "    two_dim = TSNE(random_state=1, n_jobs=-1).fit_transform(tsne_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tnse:\n",
    "\n",
    "    # t-SNE visualization\n",
    "    pd.DataFrame(two_dim).plot.scatter(x=0, y=1, c=tsne_c, colormap='tab10', figsize=(15,10))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tnse:\n",
    "\n",
    "    # t-SNE visualization\n",
    "    pd.DataFrame(two_dim).plot.scatter(x=0, y=1, c=tsne_c, colormap='tab10', figsize=(15,10))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feat importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/scikit-learn-contrib/boruta_py\n",
    "# https://towardsdatascience.com/feature-selection-with-borutapy-f0ea84c9366\n",
    "\n",
    "\n",
    "# load X and y\n",
    "# NOTE BorutaPy accepts numpy arrays only, hence the .values attribute\n",
    "# X = pd.read_csv('examples/test_X.csv', index_col=0).values\n",
    "# y = pd.read_csv('examples/test_y.csv', header=None, index_col=0).values\n",
    "# y = y.ravel()\n",
    "\n",
    "def feat_imp_boruta(X_df,y_s, boruta:True): \n",
    "    if boruta: \n",
    "        X_features = X_df.columns.to_list()\n",
    "        X = X_df.values\n",
    "        y = y_s.values\n",
    "\n",
    "        print('X.shape:', X.shape)\n",
    "\n",
    "        # define random forest classifier, with utilising all cores and\n",
    "        # sampling in proportion to y labels\n",
    "        rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n",
    "\n",
    "        # define Boruta feature selection method\n",
    "        feat_selector = BorutaPy(rf, n_estimators='auto', verbose=0, random_state=1)\n",
    "\n",
    "        # find all relevant features - 5 features should be selected\n",
    "        feat_selector.fit(X, y)\n",
    "\n",
    "        # check selected features - first 5 features are selected\n",
    "        print(feat_selector.support_)\n",
    "        # check ranking of features\n",
    "        print(feat_selector.ranking_)\n",
    "\n",
    "        # call transform() on X to filter it down to selected features\n",
    "        X = feat_selector.transform(X)\n",
    "\n",
    "\n",
    "        # impt\n",
    "        X_metadata = pd.DataFrame({\n",
    "        'features': (X_features),\n",
    "        'support_b': (feat_selector.support_) \n",
    "        #,'ranking': (feat_selector.ranking_)\n",
    "        })\n",
    "\n",
    "\n",
    "        print(X_metadata.support_b.sum())\n",
    "\n",
    "        imp_b = X_metadata.set_index('features')\n",
    "\n",
    "    else: \n",
    "        imp_b = pd.DataFrame({'support_b':[np.nan]}, index=df_normal.drop(columns='hist_labels').columns)\n",
    "        imp_b.index.set_names('features', inplace=True)\n",
    "        imp_b\n",
    "    \n",
    "    return imp_b\n",
    "\n",
    "def feat_imp_std(df, cl_name, th):\n",
    "    \n",
    "    imp = df.groupby(cl_name).mean().std().sort_values(ascending=False)\n",
    "    imp = imp.to_frame(name='std')\n",
    "    imp['std_cumsum'] = imp.cumsum() / imp.cumsum().max()\n",
    "    imp\n",
    "\n",
    "    sns.lineplot(data=imp, x=imp.index, y='std_cumsum')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "    imp['support_std'] = [True if v <=th else False for v in imp.std_cumsum]\n",
    "    imp\n",
    "    imp.index.set_names('features', inplace=True)\n",
    "    return imp \n",
    "\n",
    "\n",
    "def feat_imp(df, cl_name, cl_names_all, boruta): \n",
    "    cl_drop = [x for x in ll if x != cl_names_all]\n",
    "    \n",
    "    df = df.drop(cl_drop, errors='ignore')\n",
    "\n",
    "    \n",
    "    # boruta\n",
    "    imp_b = feat_imp_boruta(\n",
    "        X_df=df.drop(columns=cl_name, errors='ignore'),\n",
    "        y_s=df[cl_name], \n",
    "        boruta = boruta\n",
    "    )\n",
    "\n",
    "\n",
    "    # std\n",
    "    imp_std = feat_imp_std(\n",
    "        df = df,\n",
    "        cl_name = cl_name, \n",
    "        th = .8\n",
    "    )\n",
    "\n",
    "\n",
    "    # combine\n",
    "    imp_comb = pd.concat([imp_b, imp_std], 1)\n",
    "    \n",
    "    # overall support\n",
    "    cols_with_support = ['support_b', 'support_std']\n",
    "    imp_comb['support'] = imp_comb[cols_with_support].apply(lambda x: np.sum(x), axis=1)\n",
    "    \n",
    "    return imp_comb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history\n",
    "\n",
    "imp_comb_hist = feat_imp(df=df_hist_normal,cl_name='hist_labels',cl_names_all=cl_names, boruta=True)\n",
    "imp_comb_hist\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## leverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_leverage_values(x): \n",
    "    d = {}\n",
    "    d['total_donations'] = x['donations_total'].sum()\n",
    "    d['size'] = x.size\n",
    "    return pd.Series(d)\n",
    "\n",
    "def build_leverage_df(df, cl_name): \n",
    "    leverage_df = df_normal.groupby(cl_name).apply(calc_leverage_values)\n",
    "    leverage_df['total_donations_rel'] = leverage_df['total_donations'] / leverage_df['total_donations'].sum()\n",
    "    leverage_df['size_rel'] = leverage_df['size'] / leverage_df['size'].sum()\n",
    "\n",
    "    leverage_df['leverage'] = round(leverage_df['total_donations_rel'] / leverage_df['size_rel'], 2)\n",
    "    leverage_df.reset_index(inplace=True)\n",
    "    return leverage_df\n",
    "\n",
    "def build_leverage_plotdata(leverage_df, cl_name): \n",
    "\n",
    "    # bars\n",
    "    lev_plotdata = leverage_df[[cl_name, 'total_donations_rel', 'size_rel']].melt(id_vars=cl_name)\n",
    "    lev_plotdata[cl_name] = lev_plotdata[cl_name].astype(str)\n",
    "\n",
    "    # text annotations\n",
    "    max_val_per_cl = leverage_df[['total_donations_rel', 'size_rel']].max(1)\n",
    "    dist = max_val_per_cl.max() * 0.05\n",
    "    y_coord = max_val_per_cl + dist\n",
    "    y_coord_df = y_coord.reset_index(name='y_coord')\n",
    "    y_coord_df.rename(columns={'index':cl_name}, inplace=True)\n",
    "\n",
    "    lev_text = leverage_df[[cl_name, 'leverage']]\n",
    "    #lev_text.loc[:,'y_coord'] = y_coord\n",
    "    lev_text = lev_text.merge(y_coord_df, on=cl_name)\n",
    "    lev_text\n",
    "\n",
    "    return lev_plotdata, lev_text\n",
    "\n",
    "def plot_leverage(lev_plotdata, lev_text, cl_name, plot_name): \n",
    "\n",
    "    g = sns.barplot(data=lev_plotdata, x = cl_name, y='value', hue='variable')\n",
    "\n",
    "    for _, (x,s,y) in lev_text.iterrows():\n",
    "        plt.text(x=x,y=y, s=s, horizontalalignment='center')\n",
    "\n",
    "    g.set_ylim(top=g.get_ylim()[1]*1.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #print(g.get_ylim())\n",
    "    plt.savefig(os.path.join(explorations_data_path, f'leverage_{plot_name}.jpeg'), dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "def build_plot_leverage(df, cl_name, plot_name): \n",
    "    leverage_df = build_leverage_df(df, cl_name)\n",
    "    lev_plotdata, lev_text = build_leverage_plotdata(leverage_df, cl_name)\n",
    "    plot_leverage(lev_plotdata, lev_text, cl_name, plot_name)\n",
    "    return leverage_df\n",
    "\n",
    "\n",
    "leverage_df = build_plot_leverage(\n",
    "    df=df_normal, \n",
    "    cl_name='hist_labels', \n",
    "    plot_name='hist_test'\n",
    ")\n",
    "\n",
    "#leverage_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_profiles(df, label_columns, figsize, compar_titles=None):\n",
    "    \"\"\"\n",
    "    Pass df with labels columns of one or multiple clustering labels. \n",
    "    Then specify this label columns to perform the cluster profile according to them.\n",
    "    \"\"\"\n",
    "    if compar_titles == None:\n",
    "        compar_titles = [\"\"]*len(label_columns)\n",
    "        \n",
    "    sns.set()\n",
    "    fig, axes = plt.subplots(nrows=len(label_columns), ncols=2, figsize=figsize, squeeze=False)\n",
    "    for ax, label, titl in zip(axes, label_columns, compar_titles):\n",
    "        # Filtering df\n",
    "        drop_cols = [i for i in label_columns if i!=label]\n",
    "        dfax = df.drop(drop_cols, axis=1)\n",
    "        \n",
    "        # Getting the cluster centroids and counts\n",
    "        centroids = dfax.groupby(by=label, as_index=False).mean()\n",
    "        counts = dfax.groupby(by=label, as_index=False).count().iloc[:,[0,1]]\n",
    "        counts.columns = [label, \"counts\"]\n",
    "        \n",
    "        # Setting Data\n",
    "        pd.plotting.parallel_coordinates(centroids, label, color=sns.color_palette(), ax=ax[0])\n",
    "        sns.barplot(x=label, y=\"counts\", data=counts, ax=ax[1])\n",
    "\n",
    "        #Setting Layout\n",
    "        handles, _ = ax[0].get_legend_handles_labels()\n",
    "        cluster_labels = [\"Cluster {}\".format(i) for i in range(len(handles))]\n",
    "        ax[0].annotate(text=titl, xy=(0.95,1.1), xycoords='axes fraction', fontsize=13, fontweight = 'heavy') \n",
    "        ax[0].legend(handles, cluster_labels) # Adaptable to number of clusters\n",
    "        ax[0].axhline(color=\"black\", linestyle=\"--\")\n",
    "        ax[0].set_title(\"Cluster Means - {} Clusters\".format(len(handles)), fontsize=13)\n",
    "        ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=30)\n",
    "        ax[1].set_xticklabels(cluster_labels)\n",
    "        ax[1].set_xlabel(\"\")\n",
    "        ax[1].set_ylabel(\"Absolute Frequency\")\n",
    "        ax[1].set_title(\"Cluster Sizes - {} Clusters\".format(len(handles)), fontsize=13)\n",
    "    \n",
    "    #plt.subplots_adjust(hspace=0.4, top=0.90)\n",
    "    plt.tight_layout()\n",
    "    plt.title = ''\n",
    "#    plt.suptitle(\"Cluster Simple Profilling\", fontsize=23)\n",
    "\n",
    "    filename = ''.join(label_columns)\n",
    "    plt.savefig(os.path.join(explorations_data_path, f'profile_mean_{filename}.jpeg'), dpi=200)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history\n",
    "feat_supp_hist = imp_comb_hist.loc[imp_comb_hist.support > 1,].index.to_list()\n",
    "feat_supp_hist\n",
    "cl_name_hist = 'hist_labels'\n",
    "df_hist_normal[feat_supp_hist + [cl_name_hist]].groupby(cl_name_hist).mean().T\n",
    "\n",
    "my_dpi = 200\n",
    "figsize=(\n",
    "    # 15,13\n",
    "    1800/my_dpi, 1000/my_dpi\n",
    ")\n",
    "    \n",
    "cluster_profiles(\n",
    "    df = df_hist_normal[feat_supp_hist + [cl_name_hist]], \n",
    "    label_columns = [cl_name_hist], \n",
    "    figsize = figsize, #None,#(28, 13), \n",
    "    compar_titles = [\"History clustering\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Profilling each cluster (product, behavior, merged)\n",
    "cluster_profiles(\n",
    "    df = df[metric_features.to_list() + ['product_labels', 'behavior_labels', 'merged_labels']], \n",
    "    label_columns = ['product_labels', 'behavior_labels', 'merged_labels'], \n",
    "    figsize = (28, 13), \n",
    "    compar_titles = [\"Product clustering\", \"Behavior clustering\", \"Merged clusters\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
